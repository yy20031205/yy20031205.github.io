<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>mysql技术内幕之文件</title>
      <link href="/2020/10/26/DRF%E5%AD%A6%E4%B9%A0%E4%B9%8Brestful/"/>
      <url>/2020/10/26/DRF%E5%AD%A6%E4%B9%A0%E4%B9%8Brestful/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Restfull介绍"><a href="#一、Restfull介绍" class="headerlink" title="一、Restfull介绍"></a>一、Restfull介绍</h3><blockquote><p>参考资料： <a href="https://www.ruanyifeng.com/blog/2018/10/restful-api-best-practices.html" target="_blank" rel="noopener">https://www.ruanyifeng.com/blog/2018/10/restful-api-best-practices.html</a><br><a href="https://www.ruanyifeng.com/blog/2011/09/restful.html" target="_blank" rel="noopener">https://www.ruanyifeng.com/blog/2011/09/restful.html</a></p></blockquote><blockquote><p>此系列是python框架学习系列，根据相关资料学习整理，本篇是drf框架学习的第一篇。<br>REST，即Representational State Transfer的缩写，既”表现层状态转化”。<br>RESTful架构：</p></blockquote><p>（1）每一个URI代表一种资源；</p><p>（2）客户端和服务器之间，传递这种资源的某种表现层；</p><p>（3）客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。</p><h5 id="1-、格式"><a href="#1-、格式" class="headerlink" title="(1)、格式"></a>(1)、格式</h5><ul><li><code>https://api.example.com/v1/zoos</code></li></ul><h5 id="2-、资源操作类型"><a href="#2-、资源操作类型" class="headerlink" title="(2)、资源操作类型"></a>(2)、资源操作类型</h5><ol><li><code>GET(SELECT)</code>：从服务器获取资源（一项或多项）</li><li><code>POST(CREATE)</code>：在服务器新建一个资源</li><li><code>PUT(UPDATE)</code>：在服务器端更新资源（客户端需要提供改变后的完成资源）</li><li><code>PATCH(UPDATE)</code>：在服务端更新资源（客户端提供改变哪个属性），通常是部分更新</li><li><code>DELETE(DELETE)</code>：删除资源</li><li><code>OPTION</code>：获取资源哪些属性是可以改变的</li><li><code>HEAD</code>：获取资源元数据</li></ol><h5 id="3-、样例"><a href="#3-、样例" class="headerlink" title="(3)、样例"></a>(3)、样例</h5><ol><li><code>GET https://api.example.com/v1/zoos</code> :              列出所有动物园</li><li><code>POST https://api.example.com/v1/zoos</code>:             新建一个动物园</li><li><code>GET https://api.example.com/v1/zoos/id</code>:         获取某个指定动物园信息</li><li><code>PUT https://api.example.com/v1/zoos/id</code>:         更新某个指定动物园信息（提供该动物园全部信息）</li><li><code>PATCH https://api.example.com/v1/zoos/id</code>:     更新某个指定动物园信息（提供该动物园部分信息）</li><li><code>DELETE https://api.example.com/v1/zoos/id</code>:   删除某个动物园</li><li><code>GET https://api.example.com/v1/zoos/id/animal</code>:   列出某个指定动物园的所有动物</li><li><code>DELETE https://api.example.com/v1/zoos/id/animals/id</code>:删除某个指定动物园的指定动物</li></ol><h5 id="4-、状态码"><a href="#4-、状态码" class="headerlink" title="(4)、状态码"></a>(4)、状态码</h5><ol><li><code>200 OK ---[GET]</code>:成功，幂等操作</li><li><code>201 CREATED ---[POST/PUT/PATCH]</code>:新建或修改数据成功</li><li><code>202 Accept ---[*]</code>:请求进入后台排队</li><li><code>204 NO CONTENT ---[DELETE]</code>:用户删除数据成功</li><li><code>400 INVALID REQUEST ---[POST/PUT/PATCH]</code>:用户发出的请求错误，服务器没有进行相应处理，此操作幂等</li><li><code>401 Unauthorized --[*]</code>:用户没有权限（令牌，账户，密码错误）</li><li><code>403 Forbidden ---[*]</code>:用户得到授权，但是访问被禁止</li><li><code>406 Not Acceptable ---[GET]</code>:用户请求格式不可得（例如用户请求json，但只有xml）</li><li><code>410 Gone ---[GET]</code>: 请求的资源被永久删除</li><li><code>422 Unprocesable entity ---[POST/PUT/PATCH]</code>:创建一个对象时，发生验证错误</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> python学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql技术内幕之文件</title>
      <link href="/2020/05/26/mysql%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95%E4%B9%8B%E6%96%87%E4%BB%B6/"/>
      <url>/2020/05/26/mysql%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95%E4%B9%8B%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<h2 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h2><h4 id="1-1-日志文件"><a href="#1-1-日志文件" class="headerlink" title="1.1 日志文件"></a>1.1 日志文件</h4><ul><li>错误日志<ul><li>位置查看：show variables like ‘log_error’</li><li>默认文件名是当前服务器主机名.err</li></ul></li><li>二进制日志<ul><li>记录了对数据库进行更改的所有操作，若操作本身没导致数据变化，也会写入二进制日志</li><li>二进制作用：恢复，复制，审计</li><li>log_bin：默认是主机名加上二进制日志序列号</li><li>binlog_cache_size：未提交的二进制日志会记录到缓存中，事务提交，再写入二进制日志，定义这个缓存的大小，该值是基于会话的，此不能过大，因为会为每个mysql线程分配cache，也不能过小，当事务记录大于cache，则会写入一个临时文件，该值设置是否合理，可以看show status结果中Binlog_cache_disk_use（记录使用临时文件写二进制的次数）和Binlog_cache_use（记录使用缓冲写二进制日志的次数）</li><li>max_binlog_size：单个二进制文件最大值</li><li>sync_binlog：每写缓冲多少次就同步到磁盘，设置为1，则代表同步写磁盘的机制来写二进制</li><li>innodb_support_xa：日志写入磁盘，但是提交还没发生，此时宕机，重启，此次事务会被回滚，但二进制已经写入，此时数据不一致，设置此参数为1，确保数据一致</li><li>binlog_format：STAEMENT格式记录逻辑sql语句，ROW记录行更改情况</li><li>log_slave_updates</li><li>mysqlbinlog：查看二进制日志工具</li></ul></li><li>慢查询日志<ul><li>long_query_time：慢查询阈值，大于这个值会记录到日志</li><li>slow_query_log：是否开启</li><li>slow_query_log_file：日志文件位置</li><li>log_queries_not_using_indexes，开启代表没使用索引的会记录到日志</li><li>log_throttle_queries_not_using_indexes 每分钟允许记录到日志且未使用索引的语句次数，默认是0，代表没限制，为避免频繁记录导致日志不断增大，需要设置此值</li><li>mysqldumpslow: 查看slow log</li><li>log_output=FILE|TABLE，设置日志记录到文件还是一张表中</li></ul></li><li>查询日志<ul><li>默认文件名：主机名.log</li><li>general_log：是否开启</li><li>general_log_file：位置</li></ul></li></ul><h4 id="1-2-innodb存储引擎文件"><a href="#1-2-innodb存储引擎文件" class="headerlink" title="1.2 innodb存储引擎文件"></a>1.2 innodb存储引擎文件</h4><ul><li>表空间文件<ul><li>存储的数据是按表空间存放的</li><li>innodb_data_file_path=/data/ibdata1:2000M;/data2/ibdata2:2000M:autoextend,  设置使用两个文件组成一个表空间文件，若两个文件在不同磁盘，可以提高数据库性能，用完2000M后，可以自动增长，所有表数据都会记录到这个表空间中，如果设置innodb_file_per_table=1 ，则每个表一个表空间，默认是表名.ibd,单独的表空间仅存储数据文件，索引和插入缓冲都记录到默认表空间中</li><li>innodb_data_home_dir:保存位置</li></ul></li></ul><h4 id="1-3-重做日志文件"><a href="#1-3-重做日志文件" class="headerlink" title="1.3 重做日志文件"></a>1.3 重做日志文件</h4><ul><li>默认情况下，数据目录会有ib_logfile0和ib_logfile1文件，这就是redo log file</li><li>它记录存储引擎的事务日志,宕机，会利用此文件恢复到宕机之前，保证数据完整性</li><li>每个innodb存储引擎至少有一个重做日志文件组，每个组下至少两个文件，为了高可用，可以设置多个镜像日志组，将不同组放在不同磁盘上，提高日志高可用性，日志组内每个文件大小一致，以循环写入方式运行，先写第一个文件，写满再写第二个文件，写满，再切换回第一个文件</li><li>innodb_log_file_size：每个redolog大小</li><li>innodb_log_files_in_group：重做日志文件组中重做文件数量</li><li>innodb_log_group_home_dir：日志存放路径</li><li>日志文件不能设置过大，否则恢复时间会很长，不能太小，否则会导致多次切换日志文件，太小还会导致频繁发生async checkpoint，导致性能抖动</li><li>写入是先写 redo log buffer，再按一定条件写入磁盘，从buffer写入磁盘是按一个扇区大小（512字节）写入，扇区是写入的最小单位，因此，此过程不需要双写</li><li>从redo log buffer中写入磁盘条件：<ul><li>主线程会每秒写一次，不论事务是否提交</li><li>参数innodb_flush_log_at_trx_commit控制<ul><li>0: 事务提交时，并不将日志写入磁盘，而是等主线程每秒的刷写</li><li>1: 执行commit时将日志缓冲写到磁盘，伴有fsync调用，一般设置为1，保证数据持久性</li><li>2: 异步写入磁盘</li></ul></li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 专栏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql技术内幕之innodb简介</title>
      <link href="/2020/05/26/mysql%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95%E4%B9%8Binnodb%E7%AE%80%E4%BB%8B/"/>
      <url>/2020/05/26/mysql%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95%E4%B9%8Binnodb%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本系列是mysql技术内幕-innodb存储引擎一书的笔记总结</p></blockquote><h2 id="一、Innodb存储引擎"><a href="#一、Innodb存储引擎" class="headerlink" title="一、Innodb存储引擎"></a>一、Innodb存储引擎</h2><h3 id="1-1-体系架构"><a href="#1-1-体系架构" class="headerlink" title="1.1 体系架构"></a>1.1 体系架构</h3><h4 id="1-1-数据库文件"><a href="#1-1-数据库文件" class="headerlink" title="1.1.数据库文件"></a>1.1.数据库文件</h4><pre><code>.frm: myisam和innodb存储引擎的表结构文件.MYI: myisam存储引擎的表索引文件.MYD: myisam存储引擎的表数据文件.ibd: innodb存储引擎的索引文件和数据文件mysql是单进程多线程架构，插件式存储引擎，存储引擎是基于表的每张表的存储是按主键顺序进行存放，若没显式指定主键，会字动生成6字节rowid作为主键myisam只缓存索引文件，数据文件的缓存由操作系统负责mysql&gt; show engines\G </code></pre><h4 id="1-2主要线程介绍"><a href="#1-2主要线程介绍" class="headerlink" title="1.2主要线程介绍"></a>1.2主要线程介绍</h4><ul><li><p>1.2.1 后台线程</p><ul><li><p>作用：刷新内存池中数据，将修改的数据刷写到磁盘，保证mysql异常情况下能恢复到正常运行状态</p></li><li><p>master thread：核心后台线程，将缓冲池中数据异步刷新到磁盘，保证数据一致性</p></li><li><p>io thread：使用了aio来处理写io请求，负责io请求回调处理</p><pre><code>           &gt; show variables like &#39;innodb_%io_threads&#39;;            +-------------------------+-------+            | Variable_name           | Value |            +-------------------------+-------+            | innodb_read_io_threads  | 4     |            | innodb_write_io_threads | 4     |            +-------------------------+-------+           &gt; show engine innodb status\G   查看io thread情况            I/O thread 0 state: waiting for completed aio requests (insert buffer thread)            I/O thread 1 state: waiting for completed aio requests (log thread)            I/O thread 2 state: waiting for completed aio requests (read thread)            I/O thread 3 state: waiting for completed aio requests (read thread)            I/O thread 4 state: waiting for completed aio requests (read thread)            I/O thread 5 state: waiting for completed aio requests (read thread)            I/O thread 6 state: waiting for completed aio requests (write thread)            I/O thread 7 state: waiting for completed aio requests (write thread)            I/O thread 8 state: waiting for completed aio requests (write thread)            I/O thread 9 state: waiting for completed aio requests (write thread)</code></pre></li><li><p>purge thread：作用是用来回收已经使用并分配的undo页，老版本该线程在master thread中，新版本可以使用独立的，需要在配置文件中设置</p><pre><code>innodb_purge_threads=1&gt; show variables like &#39;innodb_purge_threads&#39;;    +----------------------+-------+    | Variable_name        | Value |    +----------------------+-------+    | innodb_purge_threads | 4     |    +----------------------+-------+</code></pre></li><li><p>page cleaner thread: 从master thread中独立出来，负责脏页刷新</p></li></ul></li></ul><h4 id="1-3-内存"><a href="#1-3-内存" class="headerlink" title="1.3 内存"></a>1.3 内存</h4><ul><li><p>1.3.1 缓冲池</p><ul><li><p>对页的修改，先修改缓冲池中的页，再以一定频率刷写到磁盘</p></li><li><p>刷写磁盘通过checkpoint机制完成</p></li><li><p>缓冲池中缓存的类型有：索引页，数据页，undo页，insert buffer，自适应哈希索引，锁信息，数据字典信息等</p></li><li><p>允许多个缓冲池，默认是1个</p><pre><code>&gt; show variables like &#39;innodb_buffer_pool_instances&#39;;&gt; show engine innodb status\G   查看目前缓冲池情况也可以通过information_schema中的innodb_buffer_pool_stats表查看缓存状态</code></pre></li><li><p>缓冲池管理使用改进的LRU算法</p></li><li><p>脏页：缓冲池中数据与磁盘数据不一样</p></li></ul></li><li><p>1.3.2 redo log buffer</p><ul><li>一般不需要设置太大，一般每秒钟刷新一次redo log buffer到redo log中，该值大小由innodb_log_buffer_size,默认8mb</li><li>以下三种情况，会将buffer中内容刷写到磁盘redo log中<ul><li>master thread每秒钟会刷写一次</li><li>每个事务提交时会刷写</li><li>当redo log buffer 剩余空间小于1/2，会刷写</li></ul></li></ul></li><li><p>1.3.2 额外内存池</p><ul><li>innodb对内存管理使用内存堆方式</li></ul></li></ul><h4 id="1-4-checkpoint技术"><a href="#1-4-checkpoint技术" class="headerlink" title="1.4 checkpoint技术"></a>1.4 checkpoint技术</h4><ul><li>为避免在缓冲中刷写磁盘时发生宕机，数据库普遍采用write ahead log策略，即当事务提交时，先写重做日志，再修改页。一旦宕机，通过redo log来完成数据恢复，这是acid中持久性的要求</li><li>check point解决以下问题：<ul><li>缩短数据库恢复时间</li><li>缓冲池不够时，将脏页刷写到磁盘</li><li>重做日志不可用，刷新脏页</li></ul></li><li>数据库宕机，只需对checkpoint后的重做日志进行恢复，</li><li>当进行LRU时，溢出的页若为脏页，那就要强制执行checkpoint，将脏页刷写到磁盘</li><li>重做日志是循环使用的，不需要的数据可以被覆盖</li><li>innodb通过lsn来标记版本，重做日志，每个页，checkpoint都有lsn，可以通过show engine innodb status来查看</li><li>有两种checkpoint：<ul><li>sharp checkpoint：在数据库关闭时将所有脏页都刷写到磁盘，这是默认的，即innodb_fast_shutdown=1</li><li>fuzzy checkpoint：但在数据库运行时也使用sharp checkpoint，数据库可用性会收很大影响，此时使用fuzzy checkpoint进行页刷新，只刷新部分脏页，而不是所有脏页<ul><li>以下情况会发生fuzzy checkpoint：<ul><li>master thread checkpoint</li><li>flush_lru_list checkpoint</li><li>Async/sync flush checkpoint</li><li>dirty page too much chekpoint: innodb_max_dirty_pages_pct参数控制，当脏页数大于该参数值，强制checkpoint，刷新一部分脏页到磁盘</li></ul></li></ul></li></ul></li></ul><h4 id="1-5-innodb关键特性"><a href="#1-5-innodb关键特性" class="headerlink" title="1.5 innodb关键特性"></a>1.5 innodb关键特性</h4><ul><li><p>insert buffer：在innodb中，主键是行唯一的标识符，行插入顺序是按主键递增的顺序插入，并不是所有主键插入都是顺序的，插入聚集索引一般是按顺序的，不需要磁盘的随机读取，一张表也会有辅助索引，这些索引插入不一定是顺序的</p><ul><li>对于非聚集索引的插入或更新，不是每一次直接插入到索引页中，而是先判断插入的非聚集索引是否在缓冲池中，在就直接插入，不在，则先放入一个insert buffer对象中</li><li>insert buffer使用需要同时满足两个条件：索引是辅助索引，索引不是唯一的</li></ul></li><li><p>change buffer：引入inset buffer，delete buffer， purge buffer</p></li><li><p>两次写：doublewrite</p><ul><li>数据库宕机，会通过重做日志进行恢复，但redo log中记录的是对页的物理操作，如偏移量800，写‘aaa’记录，如果这个页本身发生损坏，再进行重做是没意义的</li><li>双写就是在apply重做日志之前，需要一个页的副本，当写入失效，先通过页的副本来还原该页，再进行重做</li><li>innodb_doublewrite 可以设置是否使用doublewrite，在从服务器需要较快性能，可以关闭，需要数据高可靠性的数据库上应该开启此参数，如果文件系统提供了写失效的防范机制，也可以关闭</li></ul></li><li><p>异步io</p><h4 id="1-6-重要参数"><a href="#1-6-重要参数" class="headerlink" title="1.6 重要参数"></a>1.6 重要参数</h4></li></ul><pre><code>innodb_fast_shutdown  0:数据库关闭时，innodb需要完成所有的full purge和merge insert buffer操作，并将所有脏页刷写到磁盘。这需要一定时间，若进行innodb升级，必须设置为0，然后在关闭数据库  1:默认值，不需要完成full purge和merge insert buffer操作，但脏数据要刷写到磁盘  2: 不完成full purge和merge insert buffer操作，也不将脏页写入磁盘，而是写入日志文件。这样没有任何事务丢失，下次启动数据库，会进行恢复操作  某些情况，不需要进行完整恢复，如用户直到如何恢复，进行完整恢复时间会很长，这时可以用户自己进行恢复innodb_force_recovery：  0:默认为0，当发生需要恢复操作时，进行所有恢复操作，当不能有效恢复，如mysql crash，把错误写入到错误日志  1:忽略检查到的corrupt页  2:阻止master thread线程的运行，如master thread需要进行full purge，而这会导致mysql crash  3:不进行事务回滚操作  4:不进行insert buffer的合并操作  5:不查看undolog，innodb会将未提交的事务视为已提交  6:不进行前滚操作  设置大于0的值，可以对表进行select，create，drop。但insert，update，delete操作</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> 专栏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql技术内幕之备份恢复</title>
      <link href="/2020/05/26/mysql%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95%E4%B9%8B%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/"/>
      <url>/2020/05/26/mysql%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95%E4%B9%8B%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/</url>
      
        <content type="html"><![CDATA[<h2 id="备份与恢复"><a href="#备份与恢复" class="headerlink" title="备份与恢复"></a>备份与恢复</h2><h3 id="1-1-备份类型"><a href="#1-1-备份类型" class="headerlink" title="1.1 备份类型"></a>1.1 备份类型</h3><ul><li>按备份方法划分<ul><li>hot backup：在线备份</li><li>cold backup：停止数据库备份</li><li>warm backup：在线备份，但会对数据库操作有影响，例如加全局读锁，保证数据一致性</li></ul></li><li>按备份文件内容分：<ul><li>逻辑备份：如备份成sql语句</li><li>物理备份：完全复制数据库物理文件</li></ul></li></ul><h3 id="1-2-备份注意点"><a href="#1-2-备份注意点" class="headerlink" title="1.2 备份注意点"></a>1.2 备份注意点</h3><ul><li>备份的一致性：即备份的数据在这一时间点上是一致的<ul><li>mysqldump –single-transaction: 备份在一个长事务中完成，保证备份的一致性</li><li>每个公司根据自己的实际情况，编写备份程序，该程序应该方便设置备份方法及监控备份的结果，可以实时通知数据库管理员</li></ul></li><li>对于innodb存储引擎的冷备份，只需要备份数据库的frm文件，共享表空间文件，独立表空间文件重做日志文件，定期备份配置文件</li><li>注意磁盘空间已满会导致备份失败的情况</li></ul><h3 id="1-3-备份工具"><a href="#1-3-备份工具" class="headerlink" title="1.3 备份工具"></a>1.3 备份工具</h3><ul><li><p>mysqldump</p><pre><code>mysqldump [args] &gt; file_name--all-databases:备份所有库--database:备份指定库--single-transaction:保证备份一致性，只对innodb引擎有效，确保没有其他ddl操作，因为一致读不能隔离ddl操作--lock-tables:不能锁所有表，主要用在myisam引擎，保证备份一致性，与--single-transaction不能同时使用--lock-all-tables：锁所有表--add-drop-database:需要与--all-databases或--databases一起用，在create database之前运行drop database--master-data[=#]：为1，文件中记录change master语句，为2，则把change master语句注释，此选项会自动忽略--lock-table选项，如果没有--single-transaction选项，则会自动使用--lock-all-tables--where=where_condition：导出符合给定条件的数据，可以导出多张表，也可使用select... into outfile导出表此工具不能备份视图</code></pre></li><li><p>xtrabackup</p><ul><li>percona公司提供的开源mysql热备份工具，也可以实现增量备份</li></ul></li></ul><h3 id="1-4-二进制日志备份与恢复"><a href="#1-4-二进制日志备份与恢复" class="headerlink" title="1.4 二进制日志备份与恢复"></a>1.4 二进制日志备份与恢复</h3><ul><li><p>保证安全正确记录二进制的推荐配置：</p><pre><code>[mysqld]log-bin = mysql-binsync_binlog = 1innodb_support_xa = 1</code></pre></li><li><p>备份二进制日志前，可使用flush logs生产新日志，然后备份之前的日志</p></li><li><p>恢复二进制日志</p><pre><code>mysqlbinlog binlog.0000001 | mysql -p test  恢复单个二进制日志mysqlbinlog binlog.[0-5]* | mysql -p test 恢复多个二进制文件要恢复多个二进制日志，应该同时恢复，而不是一个一个恢复也可以下面这样做,导出为sql，然后在导入数据库mysqlbing binlog.0000001 &gt; /tmp/test.sql--start-position, --stop-position 指定偏移量恢复--start-datetime, --stop-datatime 指定时间点进行恢复</code></pre></li></ul><h3 id="1-5-复制"><a href="#1-5-复制" class="headerlink" title="1.5 复制"></a>1.5 复制</h3><ul><li>复制也可以实现备份的效果</li><li>复制的原理<ul><li>主服务器记录二进制日志，可通过show master status查看主服务器进程信息</li><li>从服务器把主服务器的二进制日志复制到自己的relay log中，从服务器有一个io线程，负责读取主服务器的binlog，还有一个sql线程，复制执行relay log，可通过show slave status查看从服务器进程信息</li><li>从服务器重放relay log</li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 专栏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux系统优化之四：如何处理大量不可中断进程和僵尸进程</title>
      <link href="/2020/04/26/linux%E4%BC%98%E5%8C%96%E4%B9%8B%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%E5%A4%84%E7%90%86/"/>
      <url>/2020/04/26/linux%E4%BC%98%E5%8C%96%E4%B9%8B%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<blockquote><p>之前的专栏讨论了cpu使用率高的问题，根据之前专栏的相关介绍，其实等待I/O的cpu使用率(简称iowait)升高，也是常见的性能问题，本文就讨论下这个问题</p></blockquote><h3 id="进程状态"><a href="#进程状态" class="headerlink" title="进程状态"></a>进程状态</h3><p><strong>top工具显示结果说明</strong></p><ul><li>top输出结果R列相关状态说明</li></ul><ul><li>R：running的缩写，表示进程在cpu就绪队列中，正在运行或者正在等待运行</li><li>D：disk sleep缩写，也就是不可中断睡眠，表示进程在与硬件交互，且在交互过程中不允许被其他进程或中断打断</li><li>Z：zombie缩写，僵尸进程，，就是进程结束了，但是其父进程还没有回收他的资源，比如进程描述符，pid等</li><li>S：interruptible sleep缩写，就是可中断睡眠，表示进程因为某个事件被系统挂起，当进程的事件发生时，它会被唤醒并进入R状态</li><li>I：idle的缩写，就是空闲状态，用在不可中断睡眠的内核线程上，硬件交互导致的不可中断进程用D表示，对于某些内核线程来说，他们可能实际并没有任何负载，用idle就是为了区分这种情况。D状态的进程会导致平均负载升高，I状态的进程不会</li></ul><p><strong>除了以上5个状态之外，还有下面两个状态</strong></p><ul><li>T：t，stopped或者traced缩写，表示进程处于暂停或跟踪状态。向一个进程发送SIGSTOP信号，该进程会变成暂停状态(stopped)，再发送SIGCONT信号，进程恢复运行，如果进程是终端里直接启动，则需要你用fg命令恢复到前台运行；当你用gdb调试一个进程时，再使用断点中断进程后，进程就变成跟踪状态(traced)，这也是一种特殊的暂停状态</li><li>X：dead缩写，表示进程已经消亡，不会在top或ps中看到此进程</li></ul><p><strong>不可中断状态，是为保证进程数据与硬件状态一致，正常情况下，不可中断状态会在短时间内结束，所以，短时的不可中断状态，算正常现象，但如果系统或硬件发送故障，进程会长时间在不可中断睡眠状态，甚至导致出现大量不可中断睡眠进程，这时就要引起注意</strong></p><p><strong>僵尸进程，是多进程应用很容易碰到的问题，正常情况，当一个进程创建了子进程，它应该通过系统调用wait()或waitpid()等待子进程结束，回收子进程资源；子进程结束，会向它的父进程发送SIGCHLD信号，所以，父进程还可以注册SIGCHLD信号处理函数，异步回收资源，如果父进程没这么做，或者子进程执行太快，父进程没来得及处理子进程状态，子进程就已经提前退出，那此时子进程就变成僵尸进程。通常僵尸进程持续时间较短，在父进程回收他的资源后就会消失，或者父进程退出后，由init进程回收后也会消亡，一旦父进程没有处理子进程的终止，还一直处于运行状态，那子进程就会一直处于僵尸状态，大量僵尸进程会用尽pid，导致新进程不能被创建，所以一定要避免</strong></p><h3 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h3><p><strong>环境说明</strong></p><ul><li>使用docker运行应用，另外需要使用dstat工具</li></ul><p><strong>操作分析</strong></p><pre><code># docker run -it -d --privileged --name=app feisky/app:iowait# ps aux |grep /approot       4030  0.0  0.0   4512  1620 pts/0    Ss+  17:18   0:00 /appSs+ : S表示可中断睡眠状态，s表示此进程是一个会话领导进程，+表示前台进程组</code></pre><p><strong>进程组：表示一组相互关联的进程，比如每个子进程都是父进程所在组成员</strong><br><strong>会话：共享同一个控制终端的一个或多个进程组，ssh登陆服务器，就会打开一个控制终端，这个终端就对应一个会话，在终端运行的命令及其它们的子进程，就构成一个个进程组，后台运行的命令，构成后台进程组，在前台运行的命令，构成前台进程组</strong></p><p><strong>运行top命令，按1，切换显示所以cpu使用情况</strong></p><pre><code>top - 17:32:57 up 12:39,  3 users,  load average: 2.65, 2.45, 1.63Tasks: 537 total,   1 running,  96 sleeping,   0 stopped, 348 zombie%Cpu0  :  0.3 us, 23.4 sy,  0.0 ni, 34.4 id, 41.8 wa,  0.0 hi,  0.0 si,  0.0 st%Cpu1  :  0.3 us, 18.3 sy,  0.0 ni, 42.2 id,  3.8 wa,  0.0 hi, 35.3 si,  0.0 stKiB Mem :  2017512 total,   503812 free,   496220 used,  1017480 buff/cacheKiB Swap:  2047996 total,  2047472 free,      524 used.  1354576 avail Mem   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND    16 root      20   0       0      0      0 S  28.9  0.0   2:08.95 ksoftirqd/1  4452 root      20   0       0      0      0 Z  22.4  0.0   0:00.68 app  4453 root      20   0       0      0      0 Z  21.4  0.0   0:00.65 app  4330 root      20   0   43192   4264   3232 R   1.0  0.2   0:02.76 top   983 root      20   0  191840  11500  10020 S   0.7  0.6   1:01.13 vmtoolsd 32400 root      20   0       0      0      0 I   0.7  0.0   0:10.08 kworker/0:0     8 root      20   0       0      0      0 I   0.3  0.0   0:18.87 rcu_sched</code></pre><p><strong>仔细观察top命令的输出结果，有以下几个特点：</strong></p><ul><li><p>load average 这个参数，1分钟，5分钟，15分钟的数据依次减小，说明平均负载正在升高，并且，1分钟内负载超过cpu个数2，说明系统可能出现了性能瓶颈</p></li><li><p>Tasks这一行，一个在运行，但僵尸进程很多，并且在不断增加，说明子进程退出时，资源没被清理，产生大量僵尸进程</p></li><li><p>cpu使用率，不高，但是iowait稍高，好像有点不正常</p></li><li><p>观察是否有处于D状态进程，由于本实验主机采用了ssd硬盘，出现iowait可能性不大，如果用机械硬盘，会有处于D状态进程，这一般是在等待io</p></li><li><p>问题汇总一下：<br>1.iowait高，导致平均负载高，甚至超过了cpu个数<br>2.僵尸进程不断增多，说明没有正确清理子进程资源</p></li></ul><p><strong>问题处理</strong></p><p><strong>因为机器配置原因，可能iowait没达到理想效果，故在此仅说一下处理思路问题</strong><br>1.使用dstat观察cpu和io情况，一般wai高，磁盘read会很大，说明wai高跟磁盘读请求有关<br>2.使用top观察处于D状态的进程，并找出这些进程的pid<br>3.使用pidstat -d  1 3  命令查看输出结果，观察一会，一般会找到问题进程</p><p><strong>进程要访问磁盘，就要使用系统调用，为找到问题根源，就需要使用strace这种跟踪系统调用的工具</strong></p><pre><code>strace -p pid  </code></pre><p><strong>运行此命令可能会报错，因为问题进程app已经处于Z状态，表示已经退出了，只不过处于僵尸态，故此会报错，但问题追踪还得继续，可以使用perf top ,perf record, perf report命令进行下一步操作，一般会找到问题根源</strong></p><p><strong>僵尸进程处理</strong></p><ul><li>此处说一下处理思路，出现僵尸进程，一般是要找出其父进程，然后在父进程中解决<ul><li>找到父进程，一般使用pstree命令<pre><code>pstree -a -p -s &lt;pid&gt;</code></pre></li></ul></li></ul><pre><code>  - 查看父进程程序代码里面，对子进程结束的处理是否正确，如有问题，进行更正极客解决**总结**- iowait高不代表io有性能瓶颈- 系统中只有io类型的进程运行时，iowait也会很高，但实际上，磁盘的读写远远没有达到性能瓶颈的程度- 碰到iowait高，先用dstat，pidstat确认是不是磁盘io问题，，然后找出导致问题出现的进程- 等待io进程一般是不可中断状态，可以用ps命令找到D状态的进程，此进程为可疑进程，在本案例中，io操作后，进程变成了僵尸进程，所以不能用strace直接分析，这种情况，我们用了perf工具，逐步发现了问题所在- 僵尸进程的排查，使用pstree找出父进程，去看父进程代码，去寻找问题所在</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> 专栏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux系统优化之四：cpu使用率高，为啥找不到元凶？</title>
      <link href="/2020/04/26/%E6%89%BE%E4%B8%8D%E5%88%B0%E5%8D%A0%E7%94%A8%E7%8E%87%E9%AB%98%E7%9A%84%E5%BA%94%E7%94%A8/"/>
      <url>/2020/04/26/%E6%89%BE%E4%B8%8D%E5%88%B0%E5%8D%A0%E7%94%A8%E7%8E%87%E9%AB%98%E7%9A%84%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<blockquote><p>当你发现系统cpu使用率很高的时候，不一定能找到相对应的高cpu使用率进程，本文就是讨论这样问题的，以一个实例逐步分析，给出解决思路</p></blockquote><h4 id="案例实验环境说明"><a href="#案例实验环境说明" class="headerlink" title="案例实验环境说明"></a>案例实验环境说明</h4><ul><li>以nginx+php服务为例，使用docker跑对应的服务</li></ul><h4 id="案例操作及分析过程"><a href="#案例操作及分析过程" class="headerlink" title="案例操作及分析过程"></a>案例操作及分析过程</h4><pre><code># docker run -it -d --name nginx -p 10000:80 feisky/nginx:sp# docker run -it -d --name phpfpm --network container:nginx feisky/php-fpm:sp# curl http:10.0.11.12:10000   10.0.11.2是宿主机地址，访问返回下面这个表示容器运行成功It works!# ab -c100 -n1000 http://10.0.11.12:10000/Concurrency Level:      100Time taken for tests:   7.439 secondsComplete requests:      1000Failed requests:        0Total transferred:      172000 bytesHTML transferred:       9000 bytesRequests per second:    134.43 [#/sec] (mean)Time per request:       743.867 [ms] (mean)Time per request:       7.439 [ms] (mean, across all concurrent requests)Transfer rate:          22.58 [Kbytes/sec] receivednginx 每秒请求次数为134# ab -c 5 -t 600 http://10.0.11.12:10000/top命令：top - 12:44:53 up 10:55,  3 users,  load average: 5.15, 2.63, 1.07Tasks: 213 total,   6 running, 115 sleeping,   0 stopped,   0 zombie%Cpu(s): 72.4 us, 20.8 sy,  0.0 ni,  4.7 id,  0.0 wa,  0.0 hi,  2.0 si,  0.0 stKiB Mem :  2017512 total,   415696 free,   548644 used,  1053172 buff/cacheKiB Swap:  2047996 total,  2047472 free,      524 used.  1288324 avail Mem   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND 32360 systemd+  20   0   33136   3760   2340 S   3.3  0.2   0:07.30 nginx 36648 daemon    20   0  336696  16456   8780 S   3.3  0.8   0:05.74 php-fpm 36656 daemon    20   0  336696  16456   8780 S   3.3  0.8   0:05.56 php-fpm 36649 daemon    20   0  336696  16520   8844 S   3.0  0.8   0:05.78 php-fpm 36664 daemon    20   0  336696  16456   8780 S   3.0  0.8   0:05.69 php-fpm 36677 daemon    20   0  336696  16456   8780 S   3.0  0.8   0:05.61 php-fpm 32276 root      20   0  109104   6324   4784 S   2.7  0.3   0:05.09 containerd-shim 36647 root      20   0   32896   6096   4848 S   2.3  0.3   0:04.56 ab 29885 root      20   0 1277672 105792  47732 S   1.3  5.2   1:03.54 dockerdcpu占用最高的才3.3%，看起来并不高，但是整体cpu，us占72.4% ，sy占20.8%，id为4.7，，用户cpu占用率很高，达到72.4%，看看各个进程cpu占用率并不高，为啥总的us会很高，# pidstat 12:49:44 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command12:49:45 PM     0     29885    0.00    0.99    0.00    0.00    0.99     1  dockerd12:49:45 PM     0     32276    1.98    0.00    0.00    0.00    1.98     0  containerd-shim12:49:45 PM   101     32360    0.00    3.96    0.00    1.98    3.96     1  nginx12:49:45 PM   101     32361    0.00    0.99    0.00    0.00    0.99     1  nginx12:49:45 PM     0     56045    0.99    1.98    0.00    0.99    2.97     1  ab12:49:45 PM     1     56046    0.00    2.97    0.00    1.98    2.97     0  php-fpm12:49:45 PM     1     56051    0.99    1.98    0.00    0.99    2.97     0  php-fpm12:49:45 PM     1     56057    0.00    2.97    0.00    1.98    2.97     0  php-fpm12:49:45 PM     1     56059    0.00    2.97    0.00    0.99    2.97     0  php-fpm12:49:45 PM     1     56072    0.00    2.97    0.00    1.98    2.97     0  php-fpm12:49:45 PM     1     61853    0.99    0.00    0.00    0.00    0.99     1  stress</code></pre><p><strong>看各个进程cpu占用率加起来远小于us，us很高，却找不到哪个进程，这是啥原因？</strong></p><ul><li>回头看看top命令输出中tasks这个指标，running为6，感觉有点多，php-fpm进程才5个，加上nginx进程，看来也差不多，但是php和nginx都处于s，就是sleep状态，而处于R状态的出现了stress进程，好像没运行此应用啊，而且观察发现stress进程号时而出现，时而消失，pid在不断变化，这说明，这些进程在不停重启，或者新进程<br>原因基本是两种：</li></ul><ol><li>进程不停崩溃重启，进程退出，被监控系统自动重启</li><li>这些进程是短时进程，，就是在其他应用内部通过exec调用外面的命令，这命令短时就结束，所以很难用top这类工具发现</li></ol><p>进程pid不断变化，看起来像是被其他进程调用的短时进程，要分析，需要找到它的父进程，可以使用pstree命令</p><pre><code># pstree |grep stress        |            |-containerd-shim-+-php-fpm-+-2*[php-fpm---sh---stress]        |            |                 |         `-2*[php-fpm---sh---stress---stress]</code></pre><p><strong>看出stress是php调用的子进程，找到父进程之后，就去看app应用的代码</strong></p><pre><code># docker cp phpfpm:/app .# grep stress -r appapp/index.php:// fake I/O with stress (via write()/unlink()).app/index.php:$result = exec(&quot;/usr/local/bin/stress -t 1 -d 1 2&gt;&amp;1&quot;, $output, $status);使用一些常用工具没发现大量stress进程，我们可以使用perf工具perf record -gperf report进行进一步排查，当然需要一定的代码功底了，这里就不详细叙述了，最后定位是由于短时进程stress导致cpu us高，但整个分析很复杂，而且需要看懂代码逻辑，对于此类问题，有一个工具可以监控，就是execsnoop,项目地址：https://github.com/brendangregg/perf-tools/blob/master/execsnoop</code></pre><p><strong>总结</strong></p><ul><li>应用里面调用其他二进制程序，这些程序运行时间比较短，通过top类工具不易发现</li><li>应用本身不停崩溃重启，而启动过程的资源初始化，很可能会占用相当多的cpu资源</li><li>对于这类进程，可以用pstree，execsnoop工具进行分析</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 专栏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux系统优化之二：cpu上下文切换</title>
      <link href="/2020/04/25/linux%E4%BC%98%E5%8C%96%E4%B9%8Bcpu%E4%B8%8A%E4%B8%8B%E6%96%87/"/>
      <url>/2020/04/25/linux%E4%BC%98%E5%8C%96%E4%B9%8Bcpu%E4%B8%8A%E4%B8%8B%E6%96%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p>linux是一个多任务操作系统，它支持远大于cpu核心数的任务同时运行，当然，这些任务实际上并不是真正同时运行，操作系统在很短时间内，将cpu资源轮流分配给这些任务，造成多任务同时运行的错觉。每个任务运行前，cpu需要知道从哪里加载，从哪里开始运行，需要系统事先设置好cpu寄存器和程序计数器，这些都是cpu在运行任务必须依赖的环境，这些也称为cpu上下文。上下文切换，先把前一个任务的cpu上下文保存起来，然后加载新任务的上下文，运行新任务，这称为上下文切换。</p></blockquote><h4 id="cpu上下文"><a href="#cpu上下文" class="headerlink" title="cpu上下文"></a>cpu上下文</h4><ul><li><p>根据任务不同，cpu上下文切换分为以下几个：</p><ul><li><p>进程上下文切换：</p><ul><li>linux按照特权等级，把进程运行空间分为内核空间和用户空间，cpu特权等级分为ring0-ring3四个环，ring0是操心系统内核空间，具有最高权限，可以访问所有资源，ring3是用户空间，只能访问受限资源，不能直接访问内存等硬件，必须通过系统调用陷入内核，由内核访问特权资源</li><li>进程可以在用户空间运行，称为进程的用户态，也可以在内核空间运行，称为进程内核态，用户态转换到内核态的转变，需要系统调用完成</li><li>一次系统调用过程，其实发生两次cpu上下文切换</li><li>进程上下文切换，是指从一个进程切换到另一个进程。系统调用过程中，一直是同一个进程在运行</li></ul></li><li><p>线程上下文切换：线程是调度的基本单位，进程是资源拥有的基本单位</p><ul><li>进程只有一个线程时，可认为进程就等于线程</li><li>当进程拥有多个线程时，这些线程会共享内存和全局变量等资源，这些资源在上下文切换时是不需要修改的</li><li>线程也有自己的私有数据，上下文切换时，这些资源需要保存的</li><li>线程上下文切换分两种情况，一是前后两个线程属于不同进程，此时资源不共享，此时与进程上下文切换是一样的，第二种情况是两个线程属于同一个进程，此时有些资源是共享的，只需要切换线程私有数据，如栈和寄存器</li></ul></li><li><p>中断上下文切换：</p><ul><li>中断会打断进程的正常调度和执行，中断处理完，会把打断的进程恢复执行</li><li>中断上下文并不涉及进程的用户态，中断上下文，只包括内核态内中断服务程序</li><li>对于同一个cpu，中断比进程有更高优先级，故此，中断上下文与进程上下文切换不会同时发生</li><li>中断上下文切换也需要消耗cpu，切换过多也会降低系统性能，这时候要去排查</li></ul></li></ul></li><li><p>上下文切换需要一定时间，这时间很短，但如果上下文切换频繁时，导致cpu将大量时间用在上下文切换上，进而大大缩短真正运行进程时间，这也是导致cpu平均负载升高的一个重要因素</p></li><li><p>什么时候会发生上下文切换？</p><ul><li>只有在进程调度的时候，才需要进行上下文切换，linux系统为每个cpu都维护了一个就绪队列，将活跃进程（正在运行和正在等待运行的进程）按优先级和等待cpu的时间排序，按调度算法，选择优先级高的进程来运行</li></ul></li><li><p>cpu调度</p><ul><li>为保证进程调度，cpu时间被划分为一段段时间片，时间片分配给进程，当一个进程时间片耗尽了，就会被系统挂起，切换到其他等待的cpu的进程运行</li><li>进程在系统资源不足时，要等资源满足后才可以运行，此时进程也会挂起，系统调度其他进程运行</li><li>进程通过sleep函数将自己主动挂起时，自然也会重新调度</li><li>当有更高优先级的进程需要运行时，当前进程也会被挂起，运行高优先级进程</li><li>发生硬件中断，进程会被中断挂起，转而执行中断程序</li></ul></li></ul><p><strong>由上面可以看出，，虽然同为上下文切换，同进程内的线程切换，要比多进程的切换消耗跟少的资源，这也正是多线程代替多进程的一个优势</strong></p><h3 id="如何查看系统上下文切换情况"><a href="#如何查看系统上下文切换情况" class="headerlink" title="如何查看系统上下文切换情况"></a>如何查看系统上下文切换情况</h3><p><strong>vmstat命令是常用的工具，命令结果如下：</strong></p><pre><code># vmstat 5procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r  b   swpd   free   buff  cache   si   so    bi     bo   in   cs us sy id   wa st 0  0      0 360340  14456 1251768    0    0    24    35   70   97  6  2 92   0  0 0  0      0 360340  14456 1251804    0    0     0     0   96  165  0  0 100  0  0 0  0      0 360276  14456 1251804    0    0     0    63   94  155  0  0 100  0  0 cs: 每秒上下文切换次数 in：每秒中断次数 r：就绪队列长度，就是正在运行和等待cpu进程数 b：处于不可中断睡眠状态的进程数</code></pre><p><strong>vmstat只给出系统总体上下文切换情况，要看每个进程情况，要使用pidstat</strong></p><pre><code># pidstat -w 508:30:36 PM   UID       PID   cswch/s nvcswch/s  Command08:30:41 PM     0         8     12.75      0.00  rcu_sched08:30:41 PM     0        11      0.20      0.00  watchdog/008:30:41 PM     0        14      0.20      0.00  watchdog/108:30:41 PM     0       321      0.20      0.00  kworker/0:1H08:30:41 PM     0       459     17.73      0.00  xfsaild/sda4cswch/s:每秒自愿上下文切换次数nvcswch/s:每秒非自愿上下文切换次数自愿上下文切换：进程无法获取所需自愿，导致的上下文切换，如：i/o，内存等系统资源不足，会发生自愿上下文切换非自愿上下文切换：由于进程时间片已到，被系统强制调度，进而发生上下文切换，如大量进程争抢cpu，就容易发生非自愿上下文切换</code></pre><h4 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h4><p><strong>使用sysbench模拟多线程切换调度情况，使用vmstat和pidstat工具进行分析，开启三个终端</strong><br><strong>第一个终端上：</strong></p><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># sysbench --threads=10 --max-time=300 threads run</span></code></pre><p><strong>第二个终端上：</strong></p><pre><code># vmstat 1procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs     us sy id wa st 8  0      0 350592  14456 1253500    0    0    22    34   72  798    6  2  92  0  0 7  0      0 350592  14456 1253500    0    0     0     0 5701 1538551 11 89  0  0  0 7  0      0 350592  14456 1253500    0    0     0     0 5674 1619777 15 86  0  0  0 7  0      0 350592  14456 1253500    0    0     0     0 5197 1563957 14 86  0  0  0 7  0      0 350592  14456 1253500    0    0     0     0 4767 1600961 14 86  0  0  0</code></pre><blockquote><p>从终端2可以看出，cs（上下文切换）上升到153万，r（就绪队列）达到7，远超cpu个数2，所以会有大量cpu竞争，us和sy加起来达到100%，系统（sy）cpu占用率达到86%，说明cpu主要被系统（内核）占用，中断次数（in）达到5701，综上所述，系统的就绪队列过长，达到7，远大于cpu个数2，这会导致大量上下文切换，大量切换进而导致cpu系统占用过高，但到底是哪个进程导致的，需要进行下一步分析<br>中断次数达到5700，要查看中断情况，需要查看/proc/interrupts,如下所示，变化最快的是RES，达到230多万，这中断类型是重调度中断，这种类型表示，唤醒空闲状态cpu来调度新任务运行，在多处理器系统中，调度器用来分散任务到不同cpu机制，通常也被称为处理器中断</p></blockquote><p><strong>查看中断情况：</strong></p><pre><code># cat /proc/interrupts....         CPU0       CPU1NMI:          0          0   Non-maskable interrupts LOC:    1490665    1283555   Local timer interrupts SPU:          0          0   Spurious interrupts PMI:          0          0   Performance monitoring interrupts IWI:          0          0   IRQ work interrupts RTR:          0          0   APIC ICR read retries RES:    2306244    2366613   Rescheduling interrupts CAL:       3081       3575   Function call interrupts TLB:        161        241   TLB shootdowns TRM:          0          0   Thermal event interrupts THR:          0          0   Threshold APIC interrupts DFR:          0          0   Deferred Error APIC interrupts MCE:          0          0   Machine check exceptions MCP:         71         72   Machine check polls HYP:          0          0   Hypervisor callback interrupts ERR:          0 MIS:          0 PIN:          0          0   Posted-interrupt notification event NPI:          0          0   Nested posted-interrupt event PIW:          0          0   Posted-interrupt wakeup event</code></pre><p><strong>在第三个终端上，使用pidstat观察</strong></p><pre><code># pidstat -w -u 108:45:20 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command08:45:21 PM     0     27328    0.00    1.00    0.00    0.00    1.00     0  kworker/u256:108:45:21 PM     0     27491    0.00    1.00    0.00    0.00    1.00     1  sshd08:45:21 PM     0     27761   28.00  100.00    0.00    0.00  100.00     1  sysbench08:45:21 PM     0     27793    0.00    1.00    0.00    3.00    1.00     1  pidstat08:45:20 PM   UID       PID   cswch/s nvcswch/s  Command08:45:21 PM     0         8     19.00      0.00  rcu_sched08:45:21 PM     0        16      2.00      0.00  ksoftirqd/108:45:21 PM     0       459     18.00      0.00  xfsaild/sda408:45:21 PM     0       983     10.00      0.00  vmtoolsd08:45:21 PM   115      1408      1.00      0.00  snmpd08:45:21 PM     0     27328    242.00      0.00  kworker/u256:108:45:21 PM     0     27400      6.00      0.00  kworker/0:008:45:21 PM     0     27491    128.00      1.00  sshd08:45:21 PM     0     27772      3.00      0.00  kworker/1:008:45:21 PM     0     27793      1.00    237.00  pidstat</code></pre><blockquote><p>从pidstat输出可以看出，sysbench的cpu系统占用率（%system）达到100%，上下文切换来自其他进程，非自愿上下文切换(nvcswch)最高的是pidstat，达到237，自愿上下文切换(cswch)最高的是kworker和sshd</p></blockquote><p><strong>pidstat默认显示进程指标数据，若要显示线程数据，要使用-t选项，所以第三个终端命令建议使用：</strong></p><pre><code># pidstat -wt -u 1</code></pre><h4 id="上下文切换啥情况才算正常"><a href="#上下文切换啥情况才算正常" class="headerlink" title="上下文切换啥情况才算正常"></a>上下文切换啥情况才算正常</h4><ul><li>主要取决于系统本身的cpu性能，如果系统上下文切换次数稳定，从几百到一万以内，但如果次数超过一万，或者次数出现数量级增长，就可能出现性能问题</li><li>需要根据上下文切换类型，做具体分析<ul><li>自愿上下文切换变多，说明进程在等待资源，可能发生io问题</li><li>非自愿上下文切换变多，说明进程被强制调度，也就是在争cpu，说明cpu成为瓶颈</li><li>中断次数变多，说明cpu被中断程序占用，还需要查看/proc/interrups文件来分析中断类型</li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 专栏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux系统优化之三：如何处理cpu占用100%</title>
      <link href="/2020/04/25/linux%E4%BC%98%E5%8C%96%E4%B9%8Bcpu%E5%8D%A0%E7%94%A8100%25%E8%A7%A3%E5%86%B3/"/>
      <url>/2020/04/25/linux%E4%BC%98%E5%8C%96%E4%B9%8Bcpu%E5%8D%A0%E7%94%A8100%25%E8%A7%A3%E5%86%B3/</url>
      
        <content type="html"><![CDATA[<h4 id="cpu使用率"><a href="#cpu使用率" class="headerlink" title="cpu使用率"></a>cpu使用率</h4><p><strong>linux通过proc这个虚拟文件系统，提供系统内部信息，/proc/stat提供的是系统cpu信息,可以使用以下命令查看：</strong></p><pre><code># cat /proc/stat |grep ^cpucpu  241187 1658 242572 4371878 374 0 554 0 0 0cpu0 134376 743 93967 2199565 172 0 286 0 0 0cpu1 106810 914 148605 2172313 202 0 267 0 0 0</code></pre><p><strong>第一列表示cpu编号，没有编号的cpu表示所有cpu，其他列表示不同场景下cpu累加节拍数，它的单位是USER_HZ，也就是10ms（1/100s），每列具体含义，可以man proc</strong></p><p><strong>cpu相关指标</strong></p><ul><li>user: us， 代表用户态cpu时间，它不包括nice时间，但包括guest时间</li><li>nice： ni，代表低优先级用户态时间，就是进程nice值调整为1-19之间的cpu时间，nice值取值范围是-20～19，数值越大，优先级越低</li><li>system：sys，代表内核态cpu时间</li><li>idle：id，代表空闲时间，他不包括等待io时间（iowait）</li><li>iowait：wa，代表等待io的cpu时间</li><li>irq：hi，代表处理硬中断cpu时间</li><li>softirq：si，代表软中断cpu时间</li><li>steal：st，当系统运行在虚拟机中时，被其他虚拟机占用的cpu时间</li><li>guest：guest，代表通过虚拟化运行其他操作系统的时间，也就是运行虚拟机的cpu时间</li><li>guest_nice：gnice，代表低优先级运行虚拟机时间</li></ul><p><strong>cpu使用率=1-空闲时间/总cpu时间，一般计算cpu使用率，会取间隔一段时间的两次值，做差后，在计算出这段时间内平均cpu使用率，性能工具给出的都是间隔一段时间内的平均cpu使用率，所以要注意间隔时间设置</strong></p><ul><li>查看cpu使用率工具<ul><li>top工具</li><li>top工具默认显示的是所有cpu使用率平均值，要看单个cpu，可以按1，依次查看每个cpu使用率</li><li>pidstat工具</li></ul></li></ul><pre><code>  pidstat 1 50:47:46 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command10:47:47 PM     0       459    0.00    0.98    0.00    0.00    0.98     1  xfsaild/sda410:47:47 PM     0     28777    0.98    1.96    0.00    0.00    2.94     0  pidstat%usr:用户态cpu使用率%system:内核态cpu使用率%guest:运行虚拟机cpu使用率%wait:等待cpu使用率%CPU:总的cpu使用率</code></pre><h4 id="cpu使用率过高分析"><a href="#cpu使用率过高分析" class="headerlink" title="cpu使用率过高分析"></a>cpu使用率过高分析</h4><p><strong>调试工具gdb，gdb调试程序过程会中断程序运行，故此，一般不允许在线上环境运行，通常是找出问题函数之后，线下借助gdb进一步分析函数内部问题</strong><br><strong>生产环境调试工具建议使用perf,第一种常见用法是 perf top，它能实时显示占用cpu时钟最多的函数或者指令，可以用来查找热点函数，使用如下：</strong></p><pre><code>perf topSamples: 642  of event &#39;cpu-clock&#39;, Event count (approx.): 90431059Overhead  Shared Object        Symbol  18.63%  [kernel]             [k] _raw_spin_unlock_irqrestore   5.58%  perf                 [.] __symbols__insert   5.00%  [kernel]             [k] mpt_put_msg_frame   3.66%  [kernel]             [k] vmw_cmdbuf_header_submit   2.65%  [kernel]             [k] finish_task_switch   2.42%  perf                 [.] rb_next   1.85%  [kernel]             [k] kallsyms_expand_symbol.constprop.1   1.74%  perf                 [.] map__process_kallsym_symbol   1.73%  [kernel]             [k] __softirqentry_text_startSamples: 采样数event:事件类型Event count：事件总数量Overhead：该符号的性能事件在所有采样中占的比例，用百分比显示Shared ：该函数或指令所在的动态共享对象，如内核，进程名，动态连接库名，内核模块名等Object：动态共享对象的类型，[.]表示用户空间可执行程序或者动态链接库，[k]表示内核空间Symbol：符号名，就是函数名，若函数名未知，则使用16禁止的地址来表示</code></pre><p><strong>第二种常见用法</strong></p><ul><li>perf record：展示系统性能信息，并提供保存功能</li><li>perf report：解析展示perf record保存的数据</li></ul><h5 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h5><p><strong>环境说明：以nginx+php的web服务为例，需要安装perf，systat，docker，ab工具，web实例使用两个docker实例实现，一个提供web服务器，一个作为web服务器的客户端，使用ab工具，给web服务增加压力请求，开启两个终端，具体操作如下：</strong></p><pre><code># docker run -it -d --name nginx -p 10000:80 feisky/nginx# docker run -it -d --name php --network container:nginx feisky/php-fpm# curl http://10.0.11.12:10000    访问宿主机地址的10000端口，显示It works!表示容器正常启动成功开启ab测试：# ab -c10 -n100 http://10.0.11.12:10000/Requests per second:    20.51 [#/sec] (mean)Time per request:       487.579 [ms] (mean)Time per request:       48.758 [ms] (mean, across all concurrent requests)Transfer rate:          3.44 [Kbytes/sec] received从结果可以看出，nginx每秒平均请求数只有20.51，这有点差，到底哪里出了问题，我们继续追查，一个终端上运行:# ab -c10 -n10000 http://10.0.11.12:10000/另一个终端运行：top按1，结果如下：top - 09:50:23 up  8:00,  5 users,  load average: 4.31, 1.51, 0.54Tasks: 204 total,   7 running, 105 sleeping,   0 stopped,   0 zombie%Cpu0  : 98.3 us,  1.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.7 si,  0.0 st%Cpu1  : 97.3 us,  2.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.7 si,  0.0 stKiB Mem :  2017512 total,   288396 free,   534180 used,  1194936 buff/cacheKiB Swap:  2047996 total,  2047472 free,      524 used.  1293320 avail Mem   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND 30877 daemon    20   0  336696  16548   8868 R  39.7  0.8   0:08.92 php-fpm 30879 daemon    20   0  336696  16484   8804 R  39.7  0.8   0:09.23 php-fpm 30876 daemon    20   0  336696  16548   8868 R  39.4  0.8   0:09.21 php-fpm 30880 daemon    20   0  336696  16484   8804 R  39.4  0.8   0:08.88 php-fpm 30878 daemon    20   0  336696  16484   8804 R  38.7  0.8   0:09.06 php-fpm 30225 root      20   0  109104   6168   4388 S   0.7  0.3   0:00.61 containerd-shim  1407 root      20   0 1055344  43452  23408 S   0.3  2.2   0:43.45 containerd  1408 Debian-+  20   0   64376  11564   7020 S   0.3  0.6   0:20.01 snmpd 29885 root      20   0 1277672 100664  47664 S   0.3  5.0   0:20.95 dockerd 30298 systemd+  20   0   33172   4276   2972 S   0.3  0.2   0:00.15 nginx 30299 systemd+  20   0   33136   3748   2392 S   0.3  0.2   0:00.06 nginx</code></pre><p><strong>可以看出，php-fpm进程使用率加起来将近200%，并且每个cpu的us达到98.3%，可以看出，正式由于php进程导致cpu使用率飙升</strong></p><p><strong>下面使用perf工具进一步分析：</strong></p><pre><code># perf top -g -p 30880   30880是php-fpm进程号按方向键切换到php-fpm，在按下回车键展开php-fpm调用关系，最终发现sqrt和add_function函数Samples: 205K of event &#39;cpu-clock&#39;, Event count (approx.): 3490439906  Children      Self  Shared Object       Symbol                                                               -   99.58%     0.00%  php-fpm             [.] zend_execute -   96.77%     4.19%  php-fpm             [.] execute_ex                                                          - 54.13% execute_ex                                                                                               - 20.41% 0x8c4a7c                                                                                                   5.20% sqrt                                                                                                - 16.71% 0x98dea3                                                                                                 - 4.73% 0x98dd97                                                                                                    4.68% add_function下面查看源码内容：拷贝代码到当前目录# docker cp php:/app .# grep sqrt -r app/app/index.php:  $x += sqrt($x);查看代码，进行修复，然后重新运行，会发现这次nginx每秒平均请求数大幅上升了，基本算正常了</code></pre><p><strong>总结</strong></p><ul><li>用户cpu和nice cpu高，说明用户进程占用了较多的cpu，所以应该着重排查用户进程的性能问题</li><li>系统cpu高，说明内核占用较多cpu，应着重排查内核线程或者系统调用的性能问题</li><li>io wait cpu高，说明等待io时间长，着重排查磁盘存储是否出现io问题</li><li>软中断和硬中断高，说明中断程序占用过多的cpu，应该着重排查内核中的中断处理程序</li><li><em>碰到cpu占用率高的问题，要借助top，pidstat工具，确认引起性能问题原因，再使用perf工具排查引起性能问题的具体函数*</em></li></ul><p><strong>可能会出现的问题</strong></p><ul><li>在centos系统上运行perf top -g -p pid，会看到16进制的东西，可能会报xxx.so错误，这一般是perf无法找到依赖的库，在分析容器应用问题经常会碰到，一般可以在容器外面把分析记录保存下来，到容器里去看结果，具体操作如下：</li><li>在宿主机运行perf record -g -p pid 执行一会，退出命令</li><li>把生成的perf.data拷贝到容器</li></ul><pre><code>docker cp perf.data php:/tmpdocker exec -it php /bin/bash安装perf等工具perf report 分析perf.data数据</code></pre><p><strong>centos系统一般会出现上面问题，ubuntu一般不会出现，perf找不到sqr函数，只看到地址，一般是因为依赖在容器内，故而perf无法找到php符号表，解决办法就是把perf.data拷贝到容器内用perf report进行分析</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> 专栏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux优化实战之一：平均负载</title>
      <link href="/2020/04/25/linux%E4%BC%98%E5%8C%96%E4%B9%8B%E5%B9%B3%E5%9D%87%E8%B4%9F%E8%BD%BD/"/>
      <url>/2020/04/25/linux%E4%BC%98%E5%8C%96%E4%B9%8B%E5%B9%B3%E5%9D%87%E8%B4%9F%E8%BD%BD/</url>
      
        <content type="html"><![CDATA[<h4 id="平均负载"><a href="#平均负载" class="headerlink" title="平均负载"></a>平均负载</h4><ul><li><p>平均负载：是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数，它和cpu使用率没直接关系</p></li><li><p>可运行状态进程： 正在使用cpu或者正在等待使用cpu的进程，就是ps命令看到的处于R状态的进程</p></li><li><p>不可中断状态：处于内核态关键流程中的进程，这些进程是不可被打断的，如等待硬件设备的io相应，ps命令看到的处于D状态的进程</p><ul><li>当一个进程向磁盘读写数据时，为保证数据一致性，在得到磁盘回复前，他是不能被其他进程或者中断打断的，这时候的进程就处于不可中断状态</li></ul></li><li><p>当平均负载为2，在2个cpu上，意味着所有cpu被完全占用，在4个cpu上，意味着cpu有50%的空闲，在1个cpu上，意味着一半进程竞争不到cpu</p></li><li><p>平均负载最理想的情况是等于cpu个数</p><blockquote><p>获取cpu个数命令：</p></blockquote><pre><code>   # grep &#39;model name&#39; /proc/cpuinfo |wc -l</code></pre></li><li><p>一般会给出1分钟，5分钟，10分钟三个时间内的负载情况</p><ul><li>如果三个时间段的值基本相同，或相差不大，那就说明系统负载很平稳</li><li>如果1分钟内的负载远小于15分钟的。说明系统最近1分钟内的负载在减少，而过去15分钟内却有很高的负载</li><li>如果1分钟内负载远大于15的，说明最近1分钟内负载在增加，这种增加可能是临时性的，也可能会持续下去，一旦1分钟内平均负载接近或超过cpu个数，就意味着系统正在发生过载问题，这就需要分析原因，进行优化了，一般平均负载高于cpu数量70%时，就要引起注意了，更好的办法是把系统的平均负载监控，根据历史数据，判断负载变化趋势，当负载有明显升高趋势时，就要进行分析了</li></ul></li><li><p>平均负载与cpu使用率：</p><ul><li>CPU使用率：单位时间内cpu繁忙情况</li><li>对cpu密集型进程，使用大量cpu会导致平均负载升高，这时平均负载与cpu使用率是一致的</li><li>io密集型进程，等待io进程会导致平均负载升高，但cpu使用率不一定高</li><li>大量等待cpu进程调度也会导致平均负载升高，此时cpu使用率也会比较高</li></ul></li></ul><h4 id="平均负载分析案例"><a href="#平均负载分析案例" class="headerlink" title="平均负载分析案例"></a>平均负载分析案例</h4><blockquote><p>使用iostat，mpstat，pidstat三个工具在Ubuntu系统上演示</p></blockquote><ul><li><p>安装压力测试工具stress和工具包sysstat</p><pre><code># apt install stress sysstat</code></pre></li><li><p>测试场景1: cpu密集型应用</p><ul><li>开启三个终端，第一个终端运行stress命令<pre><code># stress --cpu 1 --timeout 600</code></pre></li><li>第二个终端运行uptime命令<pre><code># watch -d uptime</code></pre></li><li>第三个终端，运行mpstatmingl<pre><code># mpstat -P ALL 5   监控所以cpu，没5秒输出1次数据</code></pre></li><li>第二个终端显示如下：此处会发现1分钟的平均负载会慢慢增加到1.00<pre><code>load average: 1.14, 0.95, 0.55</code></pre></li><li>第三个终端效果如下：看到正好又一个cpu使用率为100%，但iowait为0<pre><code>04:44:22 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle04:44:27 PM  all   50.10    0.00    0.10    0.00    0.00    0.00    0.00    0.00    0.00   49.8004:44:27 PM    0  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.0004:44:27 PM    1    0.20    0.00    0.20    0.00    0.00    0.00    0.00    0.00    0.00   99.60</code></pre></li><li>终端4:<pre><code># pidstat -u 5 1  间隔5秒后输出一组数据04:52:39 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command04:52:44 PM     0     20175    0.00    0.20    0.00    0.00    0.20     0  mpstat04:52:44 PM     0     21649  100.00    0.00    0.00    0.00  100.00     1  stress    stress进程cpu占用率为100%</code></pre><blockquote><p>场景1总结，终端1启动一个cpu占用100%的进程，终端2看到负载达到1.14，终端3看到一个cpu占用100%，但iowait为0，终端4，看出占用cpu 100%的是进程stress，以上可以发现导致占用率100%的进程是stress，平均负载升高是由于cpu占用率100%导致的</p></blockquote></li></ul></li><li><p>场景2: I/O密集型进程</p><ul><li><p>终端1</p><pre><code>stress -i 1 --timeout 600</code></pre></li><li><p>终端2</p><pre><code>watch -d uptemeload average: 0.87, 0.47, 0.46</code></pre></li><li><p>终端3</p><pre><code>mpstat -P ALL 5 105:05:32 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle05:05:37 PM  all    0.70    0.00   49.45    0.00    0.00    0.00    0.00    0.00    0.00   49.8505:05:37 PM    0    0.20    0.00    0.40    0.00    0.00    0.00    0.00    0.00    0.00   99.4005:05:37 PM    1    1.40    0.00   98.60    77.53    0.00    0.00    0.00    0.00    0.00    0.00 cpu系统占用98.6%，iowaite77.53%</code></pre></li><li><p>终端4</p><pre><code>pidstat -u 5 1Average:      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  CommandAverage:        0     19355    0.00    0.20    0.00    0.00    0.20     -  kworker/0:0Average:        0     22133    1.20   98.80    0.00    0.00  100.00     -  stress        占用率高由stress进程导致Average:        0     22134    0.20    0.00    0.00    0.00    0.20     -  watchAverage:        0     22144    0.00    0.20    0.00    0.00    0.20     -  mpstatAverage:        0     22840    0.00    0.20    0.00    0.00    0.20     -  pidstat</code></pre><blockquote><p>场景2总结：终端1模拟io压力，终端2显示1分钟平均负载0.87，终端3显示，一个cpu占用98.6%，来自系统占用（%sys），iowait高达77.53%，终端4，看出占用率最高的进程是stress，综上得出平均负载升高是因为iowait过高导致</p></blockquote></li></ul></li><li><p>场景3: 大量进程的场景</p><ul><li><p>终端1:</p><pre><code>stress -c 8 --timeout 600</code></pre></li><li><p>终端2:</p><pre><code>watch -d uptime7:18:05 up  1:17,  5 users,  load average: 6.58, 2.69, 1.37</code></pre></li><li><p>终端3:</p><pre><code>mpstat -P ALL 505:22:27 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle05:22:32 PM  all  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.0005:22:32 PM    0  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.0005:22:32 PM    1  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</code></pre></li><li><p>终端4:</p><pre><code>pidstat -u 5 105:19:08 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command05:19:13 PM     0     23074   24.85    0.00    0.00   74.75   24.85     0  stress05:19:13 PM     0     23075   24.65    0.00    0.00   74.55   24.65     1  stress05:19:13 PM     0     23076   24.85    0.00    0.00   74.75   24.85     1  stress05:19:13 PM     0     23077   24.85    0.00    0.00   74.95   24.85     1  stress05:19:13 PM     0     23078   24.85    0.00    0.00   74.75   24.85     1  stress05:19:13 PM     0     23079   24.85    0.00    0.00   74.35   24.85     0  stress05:19:13 PM     0     23080   24.85    0.00    0.00   74.55   24.85     0  stress05:19:13 PM     0     23081   24.85    0.00    0.00   74.55   24.85     0  stress05:19:13 PM     0     23319    0.00    0.20    0.00    0.20    0.20     0  pidstat</code></pre><blockquote><p>场景3总结：终端1模拟8个进程运行，从终端2显示结果看出，cpu 1分钟平均负载高达6.58，终端3看出两个cpu占用率全部100%，终端4看出，8个stress进程争抢2个cpu资源，cpu等待时间高达74.75%，说明cpu过载</p></blockquote></li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 专栏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL实战之三：事务隔离</title>
      <link href="/2020/04/25/mysql%E4%B9%8B%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/"/>
      <url>/2020/04/25/mysql%E4%B9%8B%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文是mysql实战专栏第三篇</p></blockquote><h4 id="事务隔离，为什么你改了我还看不见？"><a href="#事务隔离，为什么你改了我还看不见？" class="headerlink" title="事务隔离，为什么你改了我还看不见？"></a>事务隔离，为什么你改了我还看不见？</h4><ul><li>事务是要保证一组数据库操作，要么全部成功，要么全部失败，在mysql中，事务支持是在存储引擎层实现</li></ul><h4 id="隔离性与隔离级别"><a href="#隔离性与隔离级别" class="headerlink" title="隔离性与隔离级别"></a>隔离性与隔离级别</h4><ul><li><p>隔离性</p><ul><li>Atomicity:原子性</li><li>Consistency:一致性</li><li>Isolation:隔离性</li><li>Durability:持久性</li></ul></li><li><p>隔离级别</p><blockquote><p>当数据库上有多个事务同时执行的时候，就会出现以下问题，为解决以下问题，就有了隔离级别的概念</p></blockquote><ul><li><p>dirty read: 脏读</p></li><li><p>non-repeatable read： 不可重复读</p></li><li><p>phantom read：幻读</p></li><li><p>隔离级别：</p><ul><li>read uncommitted:读未提交，一个事务还没提交时，它做的变更就能被别的事务看到；别人改数据的事务尚未提交，我在我的事务中也能读到</li><li>read committed:读提交，一个事务提交之后，它做的变更才会被其他事务看到；别人改数据的事务已经提交，我在我的事务中才能读到</li><li>repeatable read:可重复读，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的，当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的,mysql默认是此隔离级别；别人改数据的事务已经提交，我在我的事务中也不去读</li><li>serializable: 串行化，对于同一行记录，写会加写锁，读会加读锁，当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行，我的事务尚未提交，别人休想改数据</li><li>以上四种隔离级别，性能依次降低，安全性依次提高</li><li>查看数据库目前隔离级别：</li></ul><pre><code> mysql&gt; show variables like &#39;transaction_isolation&#39;;  5.7版本 mysql&gt; show variables like &#39;tx_isolation&#39;; 5.6或更早版本</code></pre><ul><li>设置隔离级别，就将启动参数transaction_isolation设置为要设置的隔离级别</li><li>在mysql中，每条记录在更新时都会同时记录一条回滚记录，记录上的最新值，通过回滚操作，都可以得到前一个状态的值，同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制MVCC，系统会判断，当没有事务再需要用到回滚日志时，回滚日志会被删除</li><li>5.5版本及之前版本，回滚日志和数据字典一起放在ibdata文件里，即使长事务最终提交，回滚段被清理，文件也不会变小</li></ul></li></ul></li></ul><h4 id="事务启动方式"><a href="#事务启动方式" class="headerlink" title="事务启动方式"></a>事务启动方式</h4><blockquote><p>mysql事务启动方式有以下几种：</p></blockquote><ul><li><p>显式启动，begin或start transaction，提交语句是commit，回滚使用rollback</p></li><li><p>set autocommit=0,关闭自动提交，执行select语句，事务就启动，并不会自动提交，当执行commit或rollback，或断开连接，事务才结束，有些客户端连接成功会默认执行set autocommit=1，如果是长连接，就会导致意外的长事务，autocommit=1情况下，用begin显式启动事务，执行commit则提交事务，如果执行commit work and chain，则是提交事务并自动启动下一个事务，这样省去了在此执行begin的开销</p></li></ul><p><strong>查询超过60s的长事务</strong></p><pre><code>mysql&gt; mysql&gt; SELECT * from information_schema.INNODB_TRX WHERE TIME_TO_SEC(TIMEDIFF(NOW(),trx_started))&gt;60;</code></pre><ul><li>如何应对数据库长事务问题？<blockquote><p>此问题要从应用开发端和数据库端来看</p></blockquote><ul><li>应用开发端：<ul><li>确认是否设置autocommit=0，此确认工作可以在测试环境中开展，把mysql的general_log开启，然后跑一个业务逻辑，通过general_log日志来确认，目标是设置autocommit=1</li><li>确认是否有不必要的只读事务，有些会把好几个select语句放入事务中，这种只读事务可以去掉</li><li>业务连接数据库时，根据业务本身的预估，通过set max_execution_time命令，来控制每个语句执行的最长时间，避免单个语句意外执行长时间</li></ul></li><li>数据库端：<ul><li>监控 information_schema.innodb_trx表，设置长事务阈值，超过就报警或者kill</li><li>使用pt-kill工具</li><li>在业务功能测试阶段要输出所有general_log，分析日志，提前发现问题</li><li>如使用mysql5.6或更高版本，把innodb_undo_tablespaces设置为2，或更大值，如果真出现大事务导致回滚段过大，这样设置后清理起来更方便</li></ul></li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 专栏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL实战之二：sql语句执行之日志系统</title>
      <link href="/2020/04/24/mysql%E6%97%A5%E5%BF%97/"/>
      <url>/2020/04/24/mysql%E6%97%A5%E5%BF%97/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文是MySQL实战专栏第二篇，关于mysql日志系统</p></blockquote><h3 id="MySQL日志"><a href="#MySQL日志" class="headerlink" title="MySQL日志"></a>MySQL日志</h3><ul><li>slq更新流程会涉及两个重要日志模块<ul><li>重做日志，redo log</li><li>二进制日志 binlog</li></ul></li></ul><h4 id="重做日志-redo-log"><a href="#重做日志-redo-log" class="headerlink" title="重做日志 redo log"></a>重做日志 redo log</h4><ul><li>mysql每次更新操作都需要写磁盘，在此盘找到对应的纪录，然后更新，整个过程成本非常高，为了解决此问题，mysql采取了下面的策略：<ul><li><ol><li>write-ahead logging:有记录需要更新时，innodb引擎会先把记录写到redo log，并更新内存，存储引擎会在适当时候，将更新记录写到磁盘</li></ol></li><li><ol start="2"><li>redo log有固定大小，比如配置一组四个文件，编号为ib_logfile0，ib_logfile1，ib_logfile2，ib_logfile3每个文件大小1g，总共就是4g，从头开始写，写到末尾又回到开头写，write pos是当前记录位置，一边写，一边后移，写到3号文件末尾又回到0号开头，check point是当前要擦除的位置，也是往后移动并循环，擦除记录前要把记录更新到数据文件，write pos与checkpoint之间空着的部分，可以用来记录新纪录，如果write pos追上checkpoint，表示空间满了，此时不能再执行更新操作，得先擦除一些记录，把checkpoint推进以下</li></ol></li><li><ol start="3"><li>有了redo log innodb就可以保证数据库发生异常重启，之前的提交记录不会丢失，这个能力叫crash-safe</li></ol></li><li><ol start="4"><li>redo log是存储引擎层特有的日志</li></ol></li><li><ol start="5"><li>innodb_flush_log_at_trx_commit 参数设为1，表示每次事务的redo log都直接持久化到磁盘，保证mysql异常重启之后数据不丢失，建议设置为1</li></ol></li></ul></li></ul><h4 id="二进制日志-binglog"><a href="#二进制日志-binglog" class="headerlink" title="二进制日志 binglog"></a>二进制日志 binglog</h4><ul><li>binlog是server层日志</li><li>binlog只能用于归档，没有crash-safe能力，所以需要redo log来实现</li><li>sync_binlog设置为1，表示每次事务的binlog都持久化到磁盘，保证mysql异常重启后binlog比丢失，建议设置为1</li><li>binglog有两种模式：一种是statement，记录sql语句，另一个是row，会记录行的内容，更新前，更新后都有，记两条</li></ul><h4 id="两种日志有三点不同："><a href="#两种日志有三点不同：" class="headerlink" title="两种日志有三点不同："></a>两种日志有三点不同：</h4><ul><li>redo log是innodb存储引擎层特有，binlog是server层的，所有存储引擎都可以使用</li><li>redo log是物理日志，记录的是在某个数据页做了啥修改，binlog记录的是逻辑日志，记录语句的原始逻辑。比如，给id=2的这行的某个字段加1</li><li>redo log是循环写，binlog是追加写入，binlog文件写到一定大小会切换到下一个，并不覆盖之前的日志</li></ul><h4 id="innodb存储引擎执行update语句时的流程"><a href="#innodb存储引擎执行update语句时的流程" class="headerlink" title="innodb存储引擎执行update语句时的流程"></a>innodb存储引擎执行update语句时的流程</h4><blockquote><p>以  update t1 set c=c+1 where id =2 这条语句为例说明</p></blockquote><p>（1）执行器找存储引擎取id=2这行，如果此行数据页在内存中，就直接返回给执行器，否则，先从磁盘读到内存，然后返回<br>（2）执行器拿到这行数据，把值加1，得到新数据c+1，调用接口写入新数据<br>（3）引擎将新数据更新到内存，同时将更新记录写到redo log，此时redo log处于prepare状态，告知执行器执行完毕，随时可以提交事务<br>（4）执行器生成这个操作的binlog，并写入磁盘<br>（5）执行器调用引擎提交事务接口，引擎把刚刚写入的redo log改写成commit状态</p><ul><li><p>上面redo log的prepare和commit这两个状态就是两阶段提交</p></li><li><p>为什么要实行两阶段提交？</p><ul><li>为了让两个日志之间的逻辑一致</li><li>不使用两阶段提交，会导致数据库状态有可能与用它日志恢复出来的库状态不一致，</li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 专栏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL实战之一：一条sql查询语句如何执行</title>
      <link href="/2020/04/23/mysql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/"/>
      <url>/2020/04/23/mysql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文是极客时间mysql实战专栏的学习笔记，此文是本专栏第一篇</p></blockquote><h3 id="查询语句执行过程"><a href="#查询语句执行过程" class="headerlink" title="查询语句执行过程"></a>查询语句执行过程</h3><ul><li>MySQL分为server层和存储引擎两部分<ul><li>server层：（按执行顺序）<ul><li>连接器：管理连接，权限验证。 <ul><li>建立连接后，此连接用户拥有的权限取决于此时读到的权限，权限做了修改，不会影响已经存在连接的权限.</li><li>连接器断开连接，由wait_timeout参数控制，默认是8小时</li><li>尽量使用长链接</li><li>长链接过多，会导致内存占用过高，过大可能会被oom</li><li>定期断开长链接，是解决内存占用高的一个方案</li><li>5.7版本或更高版本，可以在执行一个大操作后，执行mysql_reset_connection来重新初始化连接资源，这也是解决内存占用高的一个方案   </li></ul></li><li>查询缓存：命中则直接返回结果<ul><li>连接建立完，就查询缓存，如执行过，执行结果会以key-value形式保存</li><li>不在缓存中，就继续下个步骤，执行完，将结果存入缓存中</li><li>不建议使用查询缓存</li><li>缓存失效非常频繁，一个表更新，此表上所有查询缓存都会被清空</li><li>对更新压力大的数据库，缓存命中率非常低</li><li>静态表适合使用查询缓存</li><li>mysql提供按需使用方式，将参数query_cache_type设置成DEMAND，表示默认不使用缓存，需要使用缓存，用类似select SQL_CACHE * from where id=10指定</li><li>mysql8删除了查询缓存功能</li></ul></li><li>分析器：词法分析，语法分析<ul><li>没命中缓存，就对sql做解析<ul><li>先做词法分析，分析出字符串是什么，代表什么</li><li>再做语法分析，判断语句是否符合语法</li></ul></li></ul></li><li>优化器：执行计划生成，索引选择<ul><li>经分析器，已经知道要做什么了，优化器决定使用哪个索引，决定表连接顺序</li></ul></li><li>执行器：操作引擎，返回结果<ul><li>通过优化器知道怎么做，执行器就要执行了</li><li>执行前会验证权限，如命中缓存，会在缓存返回结果时做权限验证</li></ul></li></ul></li><li>存储引擎层：<ul><li>插件式</li><li>负责数据的存储和提取</li></ul></li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 专栏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>时间管理</title>
      <link href="/2020/04/01/%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86/"/>
      <url>/2020/04/01/%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>此篇文章是根据国外一片博客的个人总结</strong></p></blockquote><h3 id="时间管理之60-30-10法则"><a href="#时间管理之60-30-10法则" class="headerlink" title="时间管理之60-30-10法则"></a>时间管理之60-30-10法则</h3><ul><li>时间管理的本质是分轻重缓急，在合适的时间做合适的事，学会60-30-10法则，可以让你从无序的生活中解脱出来。</li><li>将一天中60%时间用于高价值的活动，30%时间给低价值的活动，10%时间用于能让你为明天做准备的活动</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>企业级私有仓库Harbor安装部署</title>
      <link href="/2019/11/30/%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93Harbor%E5%AE%89%E8%A3%85/"/>
      <url>/2019/11/30/%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93Harbor%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h3 id="系统环境"><a href="#系统环境" class="headerlink" title="系统环境"></a>系统环境</h3><pre><code>操作系统：CentOS 7.6IP地址：10.0.11.20域名： reg.ik8s.cc安装要求：  1. 做好服务器时间同步  2. 因为是在内网使用，所以需在要访问reg.ik8s.cc的主机上配置hosts解析，或者配置内网dns进行域名解析</code></pre><h3 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h3><pre class=" language-shell"><code class="language-shell">   $ sudo yum install -y yum-utils openssl openssl-devel   $ sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo   $ sudo yum makecache   $ sudo yum install docker-ce</code></pre><h3 id="配置docker-信任我们搭建的镜像仓库，否则提交镜像会报错"><a href="#配置docker-信任我们搭建的镜像仓库，否则提交镜像会报错" class="headerlink" title="配置docker(信任我们搭建的镜像仓库，否则提交镜像会报错)"></a>配置docker(信任我们搭建的镜像仓库，否则提交镜像会报错)</h3><pre class=" language-shell"><code class="language-shell">$ sudo vim /etc/docker/daemon.json{   "insecure-registries":["reg.ik8s.cc"]}</code></pre><h3 id="安装docker-compose"><a href="#安装docker-compose" class="headerlink" title="安装docker compose"></a>安装docker compose</h3><pre class=" language-shell"><code class="language-shell">$ sudo curl -L "https://github.com/docker/compose/releases/download/1.25.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose$ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose</code></pre><h3 id="安装Harbor"><a href="#安装Harbor" class="headerlink" title="安装Harbor"></a>安装Harbor</h3><ul><li>安装包下载</li></ul><pre class=" language-shell"><code class="language-shell">$ wget https://github.com/goharbor/harbor/releases/download/v1.9.3/harbor-offline-installer-v1.9.3.tgz$ sudo tar xf harbor-offline-installer-v1.9.3.tgz</code></pre><ul><li>创建https证书<br>此处使用自签的证书</li></ul><pre class=" language-shell"><code class="language-shell">$ sudo mkdir /etc/harbor/ssl && cd /etc/harbor/ssl$ sudo openssl genrsa -out ca.key 4096$ sudo openssl req -x509 -new -nodes -sha512 -days 3650\               -subj "/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=reg.ik8s.cc" -key ca.key -out ca.crt$ sudo openssl genrsa -out reg.ik8s.cc.key 4096$ sudo openssl req -sha512 -new \               -subj "/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=reg.ik8s.cc.com" \               -key reg.ik8s.cc.key \               -out reg.ik8s.cc.csr$ sudo cat > v3.ext <<-EOFauthorityKeyIdentifier=keyid,issuerbasicConstraints=CA:FALSEkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEnciphermentextendedKeyUsage = serverAuthsubjectAltName = @alt_names[alt_names]DNS.1=reg.ik8s.ccDNS.2=ik8s.ccDNS.3=hostnameEOF$ sudo openssl x509 -req -sha512 -days 3650 \               -extfile v3.ext \               -CA ca.crt -CAkey ca.key -CAcreateserial \               -in reg.ik8s.cc.csr \               -out reg.ik8s.cc.crt$ sudo openssl x509 -inform PEM -in reg.ik8s.cc.crt -out reg.ik8s.cc.cert</code></pre><ul><li>为Harbor配置服务端证书和key<ul><li>为docker配置证书，key和CA</li><li>Docker守护程序将.crt文件解释为CA证书，并将.cert文件解释为客户端证书。因此要做一下证书转换<pre class=" language-shell"><code class="language-shell">$ sudo openssl x509 -inform PEM -in reg.ik8s.cc.crt -out reg.ik8s.cc.cert$ sudo mkdir -p /etc/docker/certs.d/reg.ik8s.cc/$ sudo cp reg.ik8s.cc.cert /etc/docker/certs.d/reg.ik8s.cc/$ sudo cp reg.ik8s.cc.key /etc/docker/certs.d/reg.ik8s.cc/$ sudo cp ca.crt /etc/docker/certs.d/reg.ik8s.cc/</code></pre></li></ul></li><li>配置Harbor配置文件</li></ul><pre class=" language-shell"><code class="language-shell"># vim harbor.ymlhostname: reg.ik8s.cc# http related confighttp:  # port for http, default is 80. If https enabled, this port will redirect to https port  port: 80# https related confighttps:# https port for harbor, default is 443  port: 443# The path of cert and key files for nginx  certificate: /etc/harbor/ssl/reg.ik8s.cc.crt  private_key: /etc/harbor/ssl/reg.ik8s.cc.key</code></pre><ul><li>使用prepare脚本进行配置</li></ul><pre class=" language-shell"><code class="language-shell">$ sudo ./prepare</code></pre><ul><li>启动服务(如果运行了docker compose服务需要先停止)</li></ul><pre class=" language-shell"><code class="language-shell">$ sudo docker-compose up -d</code></pre><h3 id="访问服务"><a href="#访问服务" class="headerlink" title="访问服务"></a>访问服务</h3><ul><li>浏览器访问：<a href="https://reg.ik8s.cc" target="_blank" rel="noopener">https://reg.ik8s.cc</a></li><li>默认用户名和密码(生产环境要修改)：<ul><li>user: admin</li><li>passwd: Harbor12345</li></ul></li></ul><blockquote><p>参考文档：<br>1.<a href="https://yq.aliyun.com/articles/110806?spm=5176.8351553.0.0.63dd1991ZH3AH5" target="_blank" rel="noopener">https://yq.aliyun.com/articles/110806?spm=5176.8351553.0.0.63dd1991ZH3AH5</a><br>2.<a href="https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md" target="_blank" rel="noopener">https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md</a><br>3.<a href="https://github.com/goharbor/harbor/blob/master/docs/configure_https.md" target="_blank" rel="noopener">https://github.com/goharbor/harbor/blob/master/docs/configure_https.md</a><br>4.<a href="https://docs.docker.com/compose/install/" target="_blank" rel="noopener">https://docs.docker.com/compose/install/</a></p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Welcome</title>
      <link href="/2019/11/26/welcome/"/>
      <url>/2019/11/26/welcome/</url>
      
        <content type="html"><![CDATA[<p>欢迎访问我的个人博客，最近正使用Hexo对博客进行重构，欢迎批评指正！</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>kubernetes网络</title>
      <link href="/2019/11/23/Kubernetes%E7%BD%91%E7%BB%9C/"/>
      <url>/2019/11/23/Kubernetes%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h3 id="Kubernete网络模型"><a href="#Kubernete网络模型" class="headerlink" title="Kubernete网络模型"></a>Kubernete网络模型</h3><ul><li>每一个pod都有唯一的一个IP</li><li>kubernetes节点上pod内都运行一个pause容器，此容器保留和保存由Pod中的所有容器共享的网络名称空间（netns），即使容器挂了，再重新创建一个新容器，PodIP也不会变</li><li>节点内通信<ul><li>每个节点上都有一个Root Netns名称空间，主网络接口eth0位于此Netns中，每个pod都有自己的网络，并且有一个虚拟以太网对将其连接到root netns。这基本上是一个网络对，一端在root netns网中，另一端在容器netns中.</li></ul></li></ul><blockquote><p>对节点上的所有Pod完成此操作。为了使这些Pod相互通信，使用了Linux以太网桥cbr0。Docker使用了一个类似的桥接器docker0,可以使用brctl show命令列出网桥。<br>  <img src="https://i.loli.net/2019/11/27/hBlGrNac2U7AzDH.png" alt="pod networknamespace"></p></blockquote><ul><li>pod1到pod2通信过程（如下图）:<ul><li>在离开pod1网的eth0并在进入根网veth-xxx。</li><li>将其传递给cbr0，后者使用ARP请求发现目的地，并说“谁拥有该IP？”</li><li>veth-yyy说具有该IP，因此网桥知道将数据包转发到何处。</li><li>数据包到达vethyyy，越过pipe-pair到达其pod2网络。</li></ul></li></ul><p>  <img src="https://i.loli.net/2019/11/27/8B2dLGJoUy7Y1kh.gif" alt="pod1 to pod2"></p><ul><li><p>节点间通信<br>假设pod1与pod4通信，如下图：</p><p><img src="https://i.loli.net/2019/11/27/AmUWyXsdS86ofYT.gif" alt="pod1 to pod4"></p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>你的深度思考能力，是如何一步步被毁掉的?</title>
      <link href="/2019/10/28/%E4%BD%A0%E7%9A%84%E6%80%9D%E8%80%83%E8%83%BD%E5%8A%9B%E6%98%AF%E5%A6%82%E4%BD%95%E8%A2%AB%E6%AF%81%E6%8E%89%E4%BA%86/"/>
      <url>/2019/10/28/%E4%BD%A0%E7%9A%84%E6%80%9D%E8%80%83%E8%83%BD%E5%8A%9B%E6%98%AF%E5%A6%82%E4%BD%95%E8%A2%AB%E6%AF%81%E6%8E%89%E4%BA%86/</url>
      
        <content type="html"><![CDATA[<ul><li><p>奶头乐：如何化解这80%的人和20%精英之间的冲突？如何消解这80%人口的多余精力和不满情绪，转移他们的注意力？当时的美国高级智囊布热津斯基认为，唯一的方法，是给这80%的人口，塞上一个「奶嘴」。<strong>让他们安于为他们量身订造的娱乐信息中，慢慢丧失热情、抗争欲望和思考的能力。</strong></p></li><li><p>奶头乐战略</p><ul><li><p>为了化解这80%普通的人和20%精英之间的冲突，某智囊提出：是给这80%的人口，塞上一个「奶嘴」。<strong>让他们安于为他们量身订造的娱乐信息中，慢慢丧失热情、抗争欲望和思考的能力。</strong></p></li><li><p>公众们将会在不久的将来，失去自主思考和判断的能力。最终他们会期望媒体为他们进行思考，并作出判断。</p></li></ul></li></ul><ol><li><p>发展<strong>发泄性的产业。</strong>具体而言，包括色情业、赌博业，发展暴力型影视剧、游戏，集中报道无休止的口水战、纠纷冲突，等等，让大众将多余的精力发泄出来。</p></li><li><p>发展<strong>满足性的产业。</strong>包括报道连篇累牍的无聊琐事——娱乐圈新闻、明星花边、家长里短，发展廉价品牌，各种小恩小惠的活动，以及偶像剧、综艺等大众化娱乐产业，让大众沉溺于享乐和安逸中，从而丧失上进心和深度思考能力。</p><p> <strong>我们创造了工具，工具反过来塑造我们，我们选择了怎样的媒体，媒体就用怎样的方式塑造我们。</strong></p></li></ol><ul><li>案例</li></ul><ol><li><p>一条 APP 推送，背后都是一个运营团队，群策群力，经过初稿、初审、复审等一堆环节，有着专业的消费者行为学作支撑，用尽各种文案技法，目的是什么呢？就是吸引你的注意力，点进去。</p></li><li><p>一款网络游戏，背后可能是几百人的团队，用最前沿的科技，最详尽的数据，精心打造 —— 目的是什么？为了创造一个虚拟空间，来消磨你的时间。目的就是为了让你沉浸进去，在观看的时候，忘掉时间的流逝。而反过来，无论是学习、阅读、思考、写作，这些事情，哪一件有着这么强大的阵势？将「触及成本」降到这么低？不存在的。这就是消费娱乐文化为我们创造的牢笼。而我们正心满意足地，一步步走进去。</p><p> <strong>一旦你习惯了这种「低成本、高回报」的刺激，你就很难去做那些「高投入」的事情了。</strong></p><p> <strong>习惯了轻而易举能获得大量愉悦感，你就会慢慢对这种愉悦感脱敏。</strong></p></li></ol><ul><li><p>危害：失去自主思考和判断的能力。最终他们会期望媒体为他们进行思考，并作出判断。</p></li><li><p>应对建议：</p><p> <strong>1. 拒绝低幼化的语言刺激</strong></p><pre><code>  日常生活中，尽量抽出一定的时间，看深度的、优秀的书籍和文章，保持自己对语言的理解和运用能力。</code></pre><p> <strong>2. 拒绝抢夺注意力的低劣产品</strong></p><pre><code>  拒绝那些肤浅的东西，多看有突破性的，不反智的，引发思考的，有诚意的，需要动脑子的 —— 《黑镜》就很不错，《权力的游戏》也还可以。</code></pre><p> <strong>3.为自己设定有意义的目标</strong></p><pre><code>  请找到一件能够带给你长期收益和幸福感的事情，把它安排进每天的日程中。比如学习，可以帮助你对抗慵常、平凡、索然无味的日常生活。让你保持头脑的清醒。</code></pre></li></ul><blockquote><p>本文整理自微信公众号：L先生说，喜欢的可以搜搜关注公众号</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 个人感悟 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李永乐视频笔记</title>
      <link href="/2019/10/28/%E6%9D%8E%E6%B0%B8%E4%B9%90%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0/"/>
      <url>/2019/10/28/%E6%9D%8E%E6%B0%B8%E4%B9%90%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>此篇是看李永乐老师视频总结的笔记，有兴趣的小伙伴可以在b站或者今日头条等社交媒体上关注李永乐老师</strong></p></blockquote><h1 id="幸存者偏差"><a href="#幸存者偏差" class="headerlink" title="幸存者偏差"></a>幸存者偏差</h1><blockquote><p><strong>过度关注“幸存了某些经历”的人事物，忽略那些没有幸存的（可能因为无法观察到），造成错误的结论。</strong></p></blockquote><ul><li>只考察了幸存者所满足的特征，从而得出结论</li><li>只看到了很多成功者所具有的相同特征，而没有看到具有同样特征的失败者</li><li>老物件很实用，这是因为留到现在还用的老物件，都是实用的，不实用的早就丢掉了</li></ul><h1 id="墨菲定律"><a href="#墨菲定律" class="headerlink" title="墨菲定律"></a>墨菲定律</h1><blockquote><p><strong>如果一件事可能发生，无论发生的可能性有多小，也一定会发生</strong></p></blockquote><ul><li>样本足够多，小概率事件就变成了必然事件</li><li>生活中案例：<ul><li>开车玩手机，如果形成习惯，总有一天会出事故的</li><li>飞机发生事故的概率1/200万，因为飞机飞行次数很多，所以总会有出事故的可能性</li><li>航天飞机虽然发射次数不多，但出事故的概率很高，远远高于飞机，这是因为航天飞机的组件很多，每个组件出问题，都会引起航天飞机的事故</li></ul></li></ul><h1 id="庞氏骗局"><a href="#庞氏骗局" class="headerlink" title="庞氏骗局"></a>庞氏骗局</h1><blockquote><p><strong>一种欺诈形式，它吸引投资者并利用后期投资者的资金向早期投资者支付利</strong></p></blockquote><ul><li>典型表现，利用新投资人的钱向老投资者支付利息，以制造赚钱的假象，进而骗取更多的投资</li><li>典型案例，就是传销<ul><li>宣传者宣称在货币流通过程中就会产生价值，其实，流通并不能产生价值，劳动才能产生价值。</li><li>传销基本没有任何的生产或者贸易，这种过程中会形成一种金字塔型结构，越下级的人越得不到钱，越上级的人越可以拿到很多钱</li></ul></li><li>如何认清这种骗局？<ul><li>提供不提供商品，或者商业模式？</li><li>利率高不高？</li><li>是否制造了投资紧迫感？</li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日常生活知识</title>
      <link href="/2019/10/25/%E6%97%A5%E5%B8%B8%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
      <url>/2019/10/25/%E6%97%A5%E5%B8%B8%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>此篇主要介绍日常生活中的一些知识</strong></p></blockquote><h2 id="关于防火知识"><a href="#关于防火知识" class="headerlink" title="关于防火知识"></a>关于防火知识</h2><h3 id="火灾"><a href="#火灾" class="headerlink" title="火灾"></a>火灾</h3><ul><li>火灾形成条件：氧气，温度，易燃物，以上三者称为火灾三角形</li><li>火灾发展过程：<ul><li>初期：燃烧</li><li>成长：可燃物会溶化，有的还会发生裂解，形成有毒物质，同时还会生成炭颗粒（烟），可能会生成一氧化碳</li><li>全盛期：温度升高到609摄氏度以上，会出现轰燃现象，导致起火面积迅速扩大</li></ul></li><li>爆燃现象：氧气驱动型的，密闭空间起火，伴随火势蔓延，氧气逐渐被消耗，生成碳和一氧化碳，如果此时密闭空间有大量氧气进入，如房间门被打开，此时，进入的氧气会与一氧化碳混合，会产生爆炸，随之产生冲击波，同时剧烈燃烧，应对灭火策略是首先对密闭空间进行降温处理<ul><li>在发生山体火灾时也可能发生爆燃现象，主要是气流变化而产生</li><li>爆燃产生条件：<ul><li>现场为通风不良的密闭空间</li><li>火势必须已维持了一段时间</li><li>必须有足够的空气引入火场</li><li>燃烧室内必需尚存有火源或环境温度高于一氧化碳燃点（609℃）</li></ul></li></ul></li></ul><h3 id="灭火方式"><a href="#灭火方式" class="headerlink" title="灭火方式"></a>灭火方式</h3><ul><li>油锅起火：不能用水灭火，可以采用隔离法灭火，如锅盖或者湿布盖住锅</li><li>酒精火，吃火锅时用的酒精，使用不规范或者用了不当产品充当酒精，可能在添加酒精时发生起火，添加酒精时最好远离锅，一旦发生火灾，导致人身上着火，应对措施是手捂住面部，身体倒地，在地上来回翻滚。</li><li>电器起火：不能用水灭火，但充电宝起火可用水，达到降低温度目的，进而灭火</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dockerfile最佳实践</title>
      <link href="/2019/10/23/dockerfile%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"/>
      <url>/2019/10/23/dockerfile%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<h2 id="dockerfile最佳实践"><a href="#dockerfile最佳实践" class="headerlink" title="dockerfile最佳实践"></a>dockerfile最佳实践</h2><ul><li><p>Docker映像由只读层组成，每个只读层代表一个Dockerfile指令。这些层是堆叠的，每个层都是上一层的变化的增量。</p></li><li><p>减少构建时间:镜像的构建顺序很重要，当你向 Dockerfile 中添加文件，或者修改其中的某一行时，那一部分的缓存就会失效，该缓存的后续步骤都会中断，需要重新构建。所以优化缓存的最佳方法是把不需要经常更改的行放到最前面，更改最频繁的行放到最后面</p></li><li><p>只拷贝需要的文件，防止缓存溢出</p></li><li><p>最小化可缓存的执行层，每一个 RUN 指令都会被看作是可缓存的执行单元。太多的 RUN 指令会增加镜像的层数，增大镜像体积，run多个命令可以写在一起，形如：RUN apt-get update &amp;&amp; apt-get install -y</p></li><li><p>删除包管理工具的缓存，在每一个 RUN 指令的末尾删除缓存。如果你在下一条指令中删除缓存，不会减小镜像的体积。</p></li><li><p>使用更具体的标签，基础镜像尽量不要使用 latest 标签，在 Dockerfile 中最好指定基础镜像的具体标签</p></li><li><p>要排除与构建无关的文件，请使用.dockerignore文件。该文件支持类似于.gitignore文件的排除模式。</p></li><li><p>使用多阶段构建，多阶段构建可以大幅度减小最终镜像的大小</p></li><li><p>不要安装不必要的软件包，镜像构建过程中生成的临时文件可以在dockerfile中使用rm删除</p></li></ul><blockquote><p>参考文档：<a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/" target="_blank" rel="noopener">https://docs.docker.com/develop/develop-images/dockerfile_best-practices/</a></p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>发布相关知识</title>
      <link href="/2019/10/23/%E7%B3%BB%E7%BB%9F%E5%8F%91%E5%B8%83%E7%9B%B8%E5%85%B3/"/>
      <url>/2019/10/23/%E7%B3%BB%E7%BB%9F%E5%8F%91%E5%B8%83%E7%9B%B8%E5%85%B3/</url>
      
        <content type="html"><![CDATA[<h3 id="预发布："><a href="#预发布：" class="headerlink" title="预发布："></a>预发布：</h3><ul><li>线上用户没有入口，只有内部工程师有入口</li><li>工程师配置host访问预发布服务器</li><li>支付等避免出现问题：打款配置1块钱，上线时要确认是否修改</li><li>发布：<ul><li>发布日期：周一，周二准备，周三发布，周四周五发现问题处理</li><li>火车头自动发布模型：定时运行，每一站例行检查，通过上车，不通过下车</li></ul></li><li>考量点：<ul><li>发布时保证缓存和数据库数据一致</li><li>对数据库操作，要锁住其他服务对数据库的修改，直到所有服务升级完再解除</li><li>观察服务日志和对应的功能点</li><li>观察线上环境日志</li></ul></li><li>app灰度发布平台：<ul><li>用户筛选：<ul><li>根据平台、版本、类别、城市</li><li>查找设备标识：IMEI openuuid token</li><li>手动上传设备标识</li></ul></li><li>发布任务<ul><li>将指定的版本app关联到指定的用户集合</li><li>支持定时</li></ul></li><li>任务管理<ul><li>管理已发布或者已下线的任务</li><li>能查看已发布任务的具体数据，如下载量等</li><li>任务进行过程中能够增加或减少目标用户集合</li><li>任务可以被暂停、上线</li></ul></li><li>版本控制<ul><li>判断设备版本号是否能升级，设备版本号要小于灰度的版本号</li><li>多个灰度版本之间能够升级</li></ul></li><li>分流模块<ul><li>判断设备是否是灰度版本目标用户</li></ul></li><li>下载模块 <ul><li>提供指定灰度版本app的下载更新地址</li></ul></li><li>灰度设计：<ul><li>协议：http+json</li></ul></li></ul></li><li>用户筛选方式：<ul><li>通过hive或者mysql数据库</li><li>前提：选择灰度目标用户时，用户量不超过百万，且都是活跃用户<ul><li>方案一：通过hive筛选，将各app日志在一个集群中，按统一格式进行清洗，查询时间较慢。筛选结果数据针对uid或者ios仍然需要再转换成token</li><li>方案二：将数据按规定格式写入mysql，数据实时或定时批量写入，定时删除</li></ul></li></ul></li><li>定时发布任务<ul><li>目的让用户在指定时间之后升级，未到时间不允许下载升级，同时在指定时间通过推送方式通知用户</li><li>方案一：发布任务后，直接将用户与灰度版本关联，并增加生效时间，同时在任务生效时，触发推送</li><li>方案二：在任务生效的指定时间，建立用户与灰度版本管理关系，同时触发推送</li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>赠别寄语</title>
      <link href="/2019/10/22/%E8%B5%A0%E5%88%AB%E5%AF%84%E8%AF%AD/"/>
      <url>/2019/10/22/%E8%B5%A0%E5%88%AB%E5%AF%84%E8%AF%AD/</url>
      
        <content type="html"><![CDATA[<blockquote><p>某老师给毕业生的寄语</p></blockquote><h3 id="致最努力最认真的你"><a href="#致最努力最认真的你" class="headerlink" title="致最努力最认真的你"></a>致最努力最认真的你</h3><ul><li>恭喜你养成了成为社会精英的习惯，这是一生的财富，你会去到最好的环境，拥有令人羡慕的资源和机会，你的人生注定艰辛，因为环境和你的内心都不允许你懈怠，而世界文明的进步，也许靠的就是你们</li><li>你要学会放下虚荣，看淡竞争，更多的发现过程中的价值，更多的关注你学会了什么，而不是你赢了没有。</li><li>你还要学会休息，要知道人生除了吃饭睡觉，其余都是小事。</li></ul><h3 id="致比上不足比下有余的你"><a href="#致比上不足比下有余的你" class="headerlink" title="致比上不足比下有余的你"></a>致比上不足比下有余的你</h3><ul><li>恭喜你活成了孔夫子认为的最智慧的样子：中庸，你遵守规则，认真上进，你也享受生活，不负青春，家长嫌你不够刻苦，你也嫌弃自己怎么一到点就眼睛涩，别人挑灯夜战，让你怀疑自己的身躯太过平凡，不过，千万不要看低自己，因为攀比的阶梯永无止境，而目前这个环境的评价体系太单一，你的乐观，你的健康，你的好胃口，好睡眠，你的交际能力你的艺术特长，都会为你的人生助力。</li><li>若说你需要什么提醒，那就是切勿让中庸变平庸，也许你应该在积极主动一点，也许你应该在独特一点，再勇敢一点，必要的休息、享受无可厚非，无聊的消磨时间却该痛改，让你的人生再闪亮一点。</li></ul><h3 id="致“偏才”、“怪才”的你"><a href="#致“偏才”、“怪才”的你" class="headerlink" title="致“偏才”、“怪才”的你"></a>致“偏才”、“怪才”的你</h3><ul><li>毫无疑问，你很聪明，你还很桀骜不驯，你好像是社会这个大机体上的变异份子，与众人格格不入，如果你足够幸运，你有宽容你的父母，有赏识你的师长，让你有自由的生长空间，可是别忘记，这两个字—生长，长相怪异，但一定别忘记生长，而不论怎样生长，同样需要努力，别人在大路上奔跑，你在羊肠小路上跋涉，最终都会抵达心中的圣殿。</li></ul><h3 id="致迷茫，暂停脚步的你"><a href="#致迷茫，暂停脚步的你" class="headerlink" title="致迷茫，暂停脚步的你"></a>致迷茫，暂停脚步的你</h3><ul><li><p>我同意不是所有人都是读书的料，如果你有兄弟姐妹，也许你不会被选中走读书之路，而可能去踢球，去做生意，去学一门手艺。不幸的是，传统文化和独子家庭必需把宝压在你身上，在你不擅长的领域，你被竞争甩在了最后，却不敢也不能言弃，苦苦支撑。师长们说你懒惰，不懂事，其实你只是迷茫。我毫不怀疑，当你走进这一段被迫跟进的道路，发现你感兴趣也擅长的领域，你会逐渐变成你做梦也没想过要变成的学霸，只不过，你比别人慢了半拍，到那时，你会惊喜的发现，不会考试的你，恰恰能突破常规，老记不住知识点的你，却有杰出的动手能力，门门功课落后的你，却在江湖上呼朋唤友，如鱼得水。</p></li><li><p>当然，老师对你并非没有忧虑，我忧虑你太快说放弃，太容易找借口，太轻易就逃避责任，做事养成了马虎的习惯。。。。。 勤奋开始的晚了一些，没关系，但无论何时一定要开始，否则，你真就遂了那些看扁你的人的愿。</p><p>  <em>祝各位同学都有美好的人生</em></p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 转载文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>书单</title>
      <link href="/2019/10/20/%E9%98%85%E8%AF%BB%E4%B9%A6%E5%8D%95-2019/"/>
      <url>/2019/10/20/%E9%98%85%E8%AF%BB%E4%B9%A6%E5%8D%95-2019/</url>
      
        <content type="html"><![CDATA[<h1 id="阅读书单"><a href="#阅读书单" class="headerlink" title="阅读书单"></a>阅读书单</h1><h3 id="技术书籍"><a href="#技术书籍" class="headerlink" title="技术书籍"></a>技术书籍</h3><ul><li>kubernetes in action（在读）</li><li>kubernetes进阶实战</li><li>持续集成与持续部署实践</li><li>zabbix企业级分布式监控系统（第二版）</li></ul><h3 id="其它书"><a href="#其它书" class="headerlink" title="其它书"></a>其它书</h3><ul><li>暗时间</li><li>清醒思考的艺术</li><li>习惯的力量</li><li>反脆弱</li></ul><blockquote><p>随后会在本博客更新对应书籍的读书笔记</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 个人感悟 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nginx 重定向配置</title>
      <link href="/2019/10/19/nginx%E4%B8%AD%E9%85%8D%E7%BD%AEredirect/"/>
      <url>/2019/10/19/nginx%E4%B8%AD%E9%85%8D%E7%BD%AEredirect/</url>
      
        <content type="html"><![CDATA[<h2 id="Nginx-301重定向解决方案"><a href="#Nginx-301重定向解决方案" class="headerlink" title="Nginx 301重定向解决方案"></a>Nginx 301重定向解决方案</h2><h3 id="非www重定向到www"><a href="#非www重定向到www" class="headerlink" title="非www重定向到www"></a>非www重定向到www</h3><pre><code>server {listen 80;server_name ik8s.cc;rewrite ^/(.*)$ http://www.ik8s.cc/$1 permanent;}</code></pre><h3 id="www重定向到非www"><a href="#www重定向到非www" class="headerlink" title="www重定向到非www"></a>www重定向到非www</h3><pre><code>server {listen 80;server_name www.ik8s.cc;rewrite ^/(.*)$ http://ik8s.cc/$1 permanent;}</code></pre><h3 id="重定向单个页面"><a href="#重定向单个页面" class="headerlink" title="重定向单个页面"></a>重定向单个页面</h3><p>有时需要重定向单个页面，以避免产生404</p><pre><code>server {...if ( $request_filename ~ oldpage/ ) {rewrite ^ http://www.ik8s.cc/newpage/? permanent;...}</code></pre><h3 id="目录重定向"><a href="#目录重定向" class="headerlink" title="目录重定向"></a>目录重定向</h3><pre><code>server {...if ( $request_filename ~ olddir/.+ ) {rewrite ^(.*) http://www.ik8s.cc/newdir/$1 permanent;...}</code></pre><h3 id="一个域名重定向到另一个域名"><a href="#一个域名重定向到另一个域名" class="headerlink" title="一个域名重定向到另一个域名"></a>一个域名重定向到另一个域名</h3><pre><code>server {...server_name example.com www.example.com; rewrite ^ $scheme://www.ik8s.cc$request_uri permanent;...}</code></pre><h3 id="一个域名重定向到另一个域名-1"><a href="#一个域名重定向到另一个域名-1" class="headerlink" title="一个域名重定向到另一个域名"></a>一个域名重定向到另一个域名</h3><p> 只能重定向到新域名的主页</p><pre><code>server {...server_name example.com www.example.com; rewrite ^ $scheme://www.ik8s.cc;...}</code></pre><blockquote><p>参考资料：<a href="https://atulhost.com/301-redirect-nginx" target="_blank" rel="noopener">https://atulhost.com/301-redirect-nginx</a></p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这些.....正在把我们变傻</title>
      <link href="/2019/10/18/%E8%BF%99%E4%BA%9B%E6%AD%A3%E6%8A%8A%E6%88%91%E4%BB%AC%E5%8F%98%E5%82%BB/"/>
      <url>/2019/10/18/%E8%BF%99%E4%BA%9B%E6%AD%A3%E6%8A%8A%E6%88%91%E4%BB%AC%E5%8F%98%E5%82%BB/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>之前看到一篇不错的文章，最近发现原来那个链接失效了，经过网络搜索，找到了这篇，在这里转载一下，做个记录</strong><br>   <strong>* 本文转载自互联网 *</strong></p></blockquote><h3 id="一、被驯化的花剌子模国王"><a href="#一、被驯化的花剌子模国王" class="headerlink" title="一、被驯化的花剌子模国王"></a>一、被驯化的花剌子模国王</h3><ul><li>“据野史记载，中亚古国花剌子模有一古怪的风俗，凡是给君王带来好消息的信使，就会得到提升，给君王带来坏消息的人则会被送去喂老虎。于是将帅出征在外，凡麾下将士有功，就派他们给君王送好消息，以使他们得到提升；有罪，则派去送坏消息，顺便给国王的老虎送去食物。”</li><li>现在，抖音等各类社交软件正在用非常高明的互联网人工智能技术，把我们训练成一个个“花剌子模国王”。</li></ul><ul><li>不知你有没有发现，当你看抖音时，只要你看过某一方面的内容，以后就会不断收到同一类型的内容，而你不感兴趣的，就不会再出现在你面前。于是，你的视野，永远被局限在一个非常狭窄的范围。我们关注的那一方面内容，就成了一口井，把我们围在中间。对于井外的一切，我们一无所知。</li></ul><h3 id="二、美国大选为什么赢的是特朗普？"><a href="#二、美国大选为什么赢的是特朗普？" class="headerlink" title="二、美国大选为什么赢的是特朗普？"></a>二、美国大选为什么赢的是特朗普？</h3><ul><li><p>哈佛大学教授凯斯·桑坦斯在《信息乌托邦》中指出：“信息传播中，公众自身的信息需求并非全方位的，公众只注意自己选择的东西和使自己愉悦的领域，久而久之，会将自身桎梏于像蚕茧一般的‘茧房’之中。”上一次的美国总统大选中，很多美国人就尝到了信息茧房的苦果。</p></li><li><p>有访谈发现：美国东部（华盛顿-纽约-波士顿一带）的教授、大学生、金融界人士，和西部（洛杉矶-旧金山-西雅图一带）的演艺界、互联网界、科技界的人士，基本上都认为希拉里稳赢，在他们看来，特朗普这个大老粗没有任何胜算。</p></li><li><p>而中部大平原的农场主、五大湖区的产业工人，却基本上都认为特朗普稳赢，在他们看来，希拉里这样的伪君子怎么可能会受到美国人欢迎呢。希拉里的拥趸们，早早就准备好了庆祝希拉里获胜的庆典和物品，就等着投票结果出来。</p></li><li><p>教授和学生们在教室里集体观看电视直播，等着最后的狂欢。结果，特朗普大获全胜。东西部的精英们全都懵了，他们呆立在教室里，久久不敢相信这个结果。他们无论如何也搞不懂，所有的人都喜欢希拉里，为什么赢的却是特朗普？</p></li><li><p>”南都观察家“特约作者冷哲的一篇文章提到了这个现象。他介绍道：一位名叫穆斯塔法的软件公司市场总监，是希拉里的忠实支持者，他的 Facebook 上充满了各种各样的支持希拉里的文章，他从来没有见过任何一篇支持特朗普的文章。他周边的朋友，也都是如此。所以他们全都坚定认为，特朗普的支持者就算是有，也是极少数。可是，当他去阅读美国总统大选期间的统计数字时才发现，就在 Facebook 上，特朗普的支持者就远超他的想象。有一篇名为《我为什么要投票给特朗普》的文章，在 Facebook 被分享了 150 万次，可他和他的朋友们全都没有听说过。穆斯塔法反思道：“我们的网络社交已经变成了一个巨大的回音室。在这里我们基本上适合有着类似观点的同伴讨论几乎一致的观点,完全未能深入理解其他社交圈子里面的观点。”</p></li></ul><h3 id="三、可怕的回音室"><a href="#三、可怕的回音室" class="headerlink" title="三、可怕的回音室"></a>三、可怕的回音室</h3><ul><li><p>试想一下，如果一个国家的外交观察家们也身处这些巨大的回音室之中，那是多么可怕的事情。</p></li><li><p>那就意味着，他们完全无法准确描述现实情况，更无法准确判断事态趋势。也就是说，他们的预测结果，可能根本与现实相反。这会给国家的外交政策带来巨大的灾难。</p></li><li><p>不要以为这是危言耸听。实际上，在美国的这次大选中，绝大部分国家的外交界和国关界的人士都预测错了——包括中国。</p></li><li><p>因为他们接触的，都是美国的政界、演艺界、知识分子、互联网企业家等精英分子。而南方州的红脖子，铁锈带的蓝领工人，他们根本就接触不到，也不屑于去接触。</p></li><li><p>预测错误的结果，是外交应对的措手不及。</p></li><li><p>例如，有的国家只准备了希拉里当选的祝贺词，结果要加班赶稿；有的国家提前派出了和希拉里友好而和特朗普不对付的庆祝团队，结果碰一鼻子灰；有的国家事先和希拉里团队打得火热，等发现特朗普当选，才发现连个牵线搭桥的人都没有……</p></li></ul><h3 id="四、一项决策需要我们综合多方信息判断"><a href="#四、一项决策需要我们综合多方信息判断" class="headerlink" title="四、一项决策需要我们综合多方信息判断"></a>四、一项决策需要我们综合多方信息判断</h3><ul><li><p>你可能会认为，只有面对这种国家大事，才需要关心这个问题，我们作为普通人，哪怕只沉迷于自己的那一口井，也不会有什么关系。</p></li><li><p>其实不然，我们日常生活中的很多决策，都需要我们综合多方面的信息去做判断。如果对世界的认识就有偏差，做出的决策，肯定会有错误。例如，你现在要做一个决定：要不要移民？如果你每天只关注微博上的某一类人，或者甚至是只翻墙看推特上的东西，你会认为，中国人民生活在水深火热之中，恨不得明天就买机票，一去不回，死后连骨灰都不要拿回来。（这当然是一种错误的观点）</p></li><li><p>而你要是恰好关注的是另一类人，或者每天只看新闻联播，你会认为，中国人民生活无限美好，超越美国指日可待，傻子才移民呢。（这当然也是一种错误的观点）显然，两方面的信息都是不准确的。中国既有不好的一面，也有好的一面。我们只有综合两方面的信息，才能做出一个更加准确的判断，使之更符合自己真实的需求。还有很多生活中、工作中的决策，都需要我们综合多方信息去判断。试举几个例子：生病了，要不要看中医？打疫苗，能不能打国产的？转基因食品能不能吃？现在要不要买房？</p></li><li><p>所有这些问题，都需要你对信息有相对较全面的判断。</p></li></ul><p>如果你只是看某一方面的信息，对另一方面的信息视而不见，或者永远怀着怀疑、批判的眼光去看与自己观点不同的信息，可能会做出偏颇的、对你不利的决策。</p><h3 id="五、被驯化的主人"><a href="#五、被驯化的主人" class="headerlink" title="五、被驯化的主人"></a>五、被驯化的主人</h3><ul><li><p>现在的互联网，人工智能推荐应用越来越广，越来越深入。每一个应用软件的背后，都有一个个庞大的团队，时时刻刻在研究我们，迎合我们，最后把我们封闭在一个个“茧房”里面。</p></li><li><p>上面举了穆斯塔法的例子，他在 Facebook 上，只能看到和自己观点相同的信息，看不到相反的信息，从而做出了错误的判断。</p></li><li><p>在中国，我们用的知乎、微博、微信等等，其实也一样。</p></li><li><p>我们只关注“三观正”的人。所谓“三观正”，就是和自己观点、价值观一致。对于那些不一致的人，我们要么永远都不会看见，要不已经取关或拉黑了。</p></li><li><p>抖音和今日头条的母公司“字节跳动”，是目前国内市场上人工智能推荐最先进的公司之一。所以抖音和头条，在制造“回音壁”这一问题上，也是最卓有成效的。</p></li><li><p>长期玩抖音的人，会越来越沉迷。</p></li><li><p>但是比这更可怕还有，是人工智能背后的算法推荐，它会让我们的认知越来越片面，人变得越来越傻。</p></li><li><p>这并不是抖音等各类社交软件的问题，而是我们人性决定的。</p></li><li><p>我们只喜欢看和自己观点一致的信息。而这些社交软件，只不过把我们不喜欢的，非常高效地屏蔽掉了。</p></li><li><p>它们取悦我们，也在驯化我们。起初，我们是主人；后来，我们是奴隶。</p></li><li><p>它们是刀子，而且是非常锋利的刀子。用得好，它们会大大提高我们切肉的效率；用得不好，会把我们自己割得遍体鳞伤。</p></li><li><p>工具本身没有问题，是我们在用错误的方式使用它们。</p></li><li><p>在这次全国政协会议上，政协委员白岩松提出，要警惕沉迷于“投你所好式”网络，并把它上升到“民族危险”的高度。</p></li><li><p>这并不是危言耸听。美国社会就已经被移动互联网时代深深割裂。自由主义和保守主义，成为互相不可逾越的鸿沟。政治家每天把大量精力陷入到党争之中，而不是去讨论具体的社会和经济政策。人与人之间的隔阂，也越来越深。在很多深蓝州（民主党大本营，一般反对特朗普），孩子们在学校不能表达支持特朗普的观点，否则就会被孤立，被歧视。</p></li><li><p>中国的网络上，也是如此。每到热点事件一出来，“五毛”和“公知”骂战必起，情绪化的渲染遍布全网，而理性的探讨却沉没深海。大家都忙着站队，对反方的观点，从来都是不屑一顾，最多只当成批判的靶子。所以，不管是左营还是右营，我们都会震惊于对方之愚蠢。我们打破脑袋也想不通，人怎么能无知、无耻到这种地步。最后，我们往往是骂一句“傻×”，然后取关或者拉黑了事，眼不见为净。其实，不是对方愚蠢，是我们把自己埋进了“茧房”，网络世界已经没有了理性探讨的土壤，所有人都变傻了。</p></li></ul><h3 id="六、如何避免变傻"><a href="#六、如何避免变傻" class="headerlink" title="六、如何避免变傻"></a>六、如何避免变傻</h3><ul><li><p>怎么才能避免抖音、微博、知乎等社交平台让我们越变越傻呢？</p></li><li><p>首先，我们要认识到，我们会因为使用这些信息渠道而变傻。</p></li><li><p>如果你觉得，“我每天都能在这上面学到很多新的知识，获得很多新的信息，我在变得更加渊博，更加智慧啊。”那就完蛋了。无知和傲慢是阻碍我们获得新知的最大障碍。你需要意识到，你每天看的这些信息，都是被一个巨大的过滤器滤过的，筛到你这里的时候，已经是很偏颇的一小部分了。</p></li><li><p>如果你以为，你之所见就是全世界，你就会常常被误导，并且被消费，被当成韭菜收割。</p></li><li><p>你常常会在热点事件出来时后，情绪激昂，然后过几天又被打脸。等下一次事件到来，你又重复这一过程。</p></li><li><p>你不断被意见领袖、营销号牵着鼻子走，他们让你笑你就笑，让你哭你就哭，你被他们卖掉还帮他们数钱，然后他们还在屏幕后面笑你“傻×”。</p></li><li><p>其次，认识到我们偏颇的选择会使我们变傻之后，我们就需要做出信息选择调整了。</p></li><li><p>万维钢在《别想说服我！》一文中，介绍了美国技术活动家 Johnson 在《信息食谱》书中提到的两条核心建议：</p><ol><li><p>我们要主动，刻意地去消费某些信息，哪怕我们不喜欢。</p></li><li><p>我们要去获取新的信息，而不是去为自己的旧观点寻找支撑。</p></li></ol></li><li><p>要做到这两条，其实并不难，但是也很难。</p></li><li><p>不难在于，它非常容易操作。</p></li><li><p>例如，在微博、今日头条上，你关注共青团中央的同时，也关注一下美国驻华大使馆；关注李开复的同时，也关注一下胡锡进；关注崔永元的同时，也关注一下司马南；关注布尔费末的同时，也关注一下李子暘……</p></li><li><p>总而言之，你的关注列表里，既要有公知，也要有五毛，既要有左派，又要有右派。如此一来，同样一件事情，你基本上总能得到两方面的信息。很难在于，你常常会很痛苦。因为你总会看到你特别不喜欢的信息，不爽到让你怀疑人生。</p></li><li><p>所以，如果你为了学习、了解信息、增进知识，它就不难。如果你纯粹是为了消遣，它就很难。但是，假如你想对这个世界多了解一点，未来的决策更妥当一点，生活更舒服一点，投资更合适一点，那就应该忍着恶心，克服困难，坚持做到这两点。</p></li><li><p>硅谷投资人王川在谈到如何做投资时说：要在各行各业，关心一些最高手的观点和思维模型，即使可能你不喜欢他，也要定期查看一下，知道他是怎么想的。这样可以八九不离十的抓住主要矛盾了。</p></li><li><p>最后，对于有更高求知需求的人，这里介绍几个非常好的，关于如何打败人工智能推荐的方法（对此不感兴趣的可直接跳到最后）。</p></li><li><p>这是杨滢博士总结的（她是清华出国的匹兹堡大学博士，曾在卡耐基梅隆大学做博士后研究，是知名的脑科学专家。）</p></li><li><p>杨滢说，想要打败推荐算法，需要两个因素：</p><ol><li><p>你需要有追求高品质内容的需求。</p></li><li><p>你需要随机取样人类各个领域的知识。</p></li></ol></li><li><p>据此，她提出了几个可马上操作的建议：</p><ol><li><p>有一个chrome应用叫stumble upon，装上以后，它会给你随机选择一些高质量网站，可能是你从来没有见过的东西。</p></li><li><p>你可以把维基百科（Wikipedia）设称自己的默认页，并且选择“随机浏览”模式（random wiki），这样每次打开浏览器就可以随机弹出一个wiki页面。</p></li><li><p>你可以去wolfram alpha上面点“给我惊喜”（surprise me），它会弹出一些有趣的知识</p></li><li><p>你可以装一个应用叫“一亿本书”（100 million books，在chrome上面），它会随机推荐一些书，直接链接到亚马逊书店，你可以看评论，非常有趣。</p></li></ol></li></ul><h3 id="七、不要当社交软件的奴隶"><a href="#七、不要当社交软件的奴隶" class="headerlink" title="七、不要当社交软件的奴隶"></a>七、不要当社交软件的奴隶</h3><ul><li><p>移动互联时代，信息就是最重要的资产。获取信息的能力，就是你最重要的能力。</p></li><li><p>如果你只能得到很少的信息，或者很偏颇的信息，你就会慢慢落后于他人。</p></li><li><p>世界很大、很美、有很多机会，请不要把自己局限在一口井、一个茧里面。</p></li><li><p>抖音和今日头条，都是很好的发明；手机和人工智能推荐，都是很好的工具。</p></li><li><p>但是，工具生产出来，应该被用于服务人类，而不是奴役人类。</p></li><li><p>请不要变成社交软件的奴隶，和移动互联时代的傻瓜。</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 转载文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于学习技术相关问题</title>
      <link href="/2019/10/11/%E5%85%B3%E4%BA%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"/>
      <url>/2019/10/11/%E5%85%B3%E4%BA%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h2 id="如何学习新技术的？"><a href="#如何学习新技术的？" class="headerlink" title="如何学习新技术的？"></a>如何学习新技术的？</h2><ul><li>学习是为了找到通往答案的路径和方法，是为了培养无师自通的能力</li></ul><h3 id="学习途径"><a href="#学习途径" class="headerlink" title="学习途径"></a>学习途径</h3><ul><li>通过搜索引擎，了解该技术基本原理是什么，具体应用场景是什么，解决什么问题？</li><li>学习资料: 国内外blog，官方文档，相关的经典书</li><li>学的时候，重视原理，体系化，实战应用，技术生态圈之间的关联</li></ul><h3 id="学习新技术的思考点"><a href="#学习新技术的思考点" class="headerlink" title="学习新技术的思考点"></a>学习新技术的思考点</h3><ul><li>技术出现的初衷是什么？</li><li>此技术为了要解决什么样的问题？</li><li>为什么那个问题要用这种解决方法？</li><li>能不能用别的方法解决？</li><li>能不能再简单一些？</li></ul><h2 id="对加班怎么看？"><a href="#对加班怎么看？" class="headerlink" title="对加班怎么看？"></a>对加班怎么看？</h2><ul><li>在特定时候，比如发版本、突然故障等， 但不支持低效率的常态加班。</li><li>一个人一天能专注的高效率时间很短，长时间加班会导致因长时间工作而导致一些技术问题，再用加班解决这些问题，从而陷入恶性循环，不能为了加班而加班。</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 个人感悟 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis系列</title>
      <link href="/2019/10/10/redis%E6%8C%81%E4%B9%85%E5%8C%96-2019/"/>
      <url>/2019/10/10/redis%E6%8C%81%E4%B9%85%E5%8C%96-2019/</url>
      
        <content type="html"><![CDATA[<h1 id="redis的持久化"><a href="#redis的持久化" class="headerlink" title="redis的持久化"></a>redis的持久化</h1><blockquote><p>redis是一个内存数据库，就是将数据内容存储在内存中，这与传统关系型数据库直接将数据保存到硬盘中相比，数据读取速度要比传统数据库快很多。但保存在内存中有个缺点，就是一旦系统宕机或者重启，那么内存中的数据就会全部丢失。redis提供了将内存数据持久化到硬盘的方式。Redis 支持两种形式的持久化，一种是RDB快照（snapshotting），另外一种是AOF（append-only-file），本篇主要介绍redis持久化内容</p></blockquote><h3 id="RDB简介"><a href="#RDB简介" class="headerlink" title="RDB简介"></a>RDB简介</h3><ul><li>RDB是对redis中的数据执行周期性的持久化,把当前内存中的数据集快照写入磁盘，也就是 Snapshot 快照（数据库中所有键值对数据）。恢复时是将快照文件直接读到内存里。</li></ul><h3 id="RDB工作流程"><a href="#RDB工作流程" class="headerlink" title="RDB工作流程"></a>RDB工作流程</h3><pre><code>1.redis根据配置自己尝试去生成rdb快照文件2.fork一个子进程出来3.子进程尝试将数据dump到临时的rdb快照文件中4.完整的rdb快照文件生成之后，就替换之前的旧的快照文件</code></pre><h3 id="RDB触发方式"><a href="#RDB触发方式" class="headerlink" title="RDB触发方式"></a>RDB触发方式</h3><ul><li>自动触发<ul><li>配置文件内用save，格式为”save m n”,表示m秒内数据存在n次修改时，自动触发bgsave</li></ul></li><li>手动触发<ul><li>save 该命令会阻塞当前Redis服务器，执行save命令期间，Redis不能处理其他命令，直到RDB过程完成为止。显然该命令对于内存比较大的实例会造成长时间阻塞，这是致命的缺陷，为了解决此问题，Redis提供了第二种方式。</li><li>bgsave 执行该命令时，Redis会在后台异步进行快照操作，执行快照同时还可以响应客户端请求。具体操作是Redis进程执行fork操作创建子进程，RDB持久化过程由子进程负责，完成后自动结束。阻塞只发生在fork阶段，一般时间很短。redis内部所有rdb操作都是采用bgsave</li></ul></li></ul><h3 id="恢复数据"><a href="#恢复数据" class="headerlink" title="恢复数据"></a>恢复数据</h3><ul><li>将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可，redis就会自动加载文件数据至内存了。Redis 服务器在载入 RDB 文件期间，会一直处于阻塞状态，直到载入工作完成为止。</li></ul><h3 id="停止rdb"><a href="#停止rdb" class="headerlink" title="停止rdb"></a>停止rdb</h3><ul><li>在redis.conf 中，注释掉所有的 save 行来停用快照功能或者直接一个空字符串来实现停用：save “”</li><li>命令行配置<br><code>&gt;config set save &quot; &quot;</code></li></ul><h3 id="RDB优点"><a href="#RDB优点" class="headerlink" title="RDB优点"></a>RDB优点</h3><pre><code>1.RDB会生成多个数据文件，每个数据文件都代表某一时刻redis中的数据，此种方式，非常适合做冷备份，可以将这种完整的数据文件放到安全存储上面去，如s3或者阿里云的odps2.RDB对redis对外提供的读写服务，影响非常小，可以让redis保持高性能，因为redis主进程只需要fork一个子进程，让子进程执行磁盘io操作来进行rdb持久化3.相对于AOF持久化机制，直接基于RDB数据文件来重启和恢复redis进程更加快速</code></pre><h3 id="RDB缺点"><a href="#RDB缺点" class="headerlink" title="RDB缺点"></a>RDB缺点</h3><pre><code>1.想在redis故障时，尽可能丢失最少的数据，RDB没有AOF好，一般来说RDB的快照文件都是每隔5分钟，或者更长时间生成一次数据，一旦故障，会丢失最近5分钟的数据2.RDB每次在fork子进程来执行RDB快照时，如果数据文件很大，可能会导致对客户端提供的服务暂停数毫秒，甚至数秒3.最大缺点是不适合做第一优先的恢复方案，如果依赖RDB做第一恢复方案，数据丢失会比较多</code></pre><h3 id="AOF-简介"><a href="#AOF-简介" class="headerlink" title="AOF 简介"></a>AOF 简介</h3><pre><code>- AOF 是通过保存Redis服务器所执行的写命令来记录数据库状态。对redis中每条写入命令做日志，以append-only的模式写入一个日志文件中，在redis重启时，通过回放AOF日志中的写入命令来重建数据集- 现代操作系统中，写文件不是直接写入磁盘的，会先写 os cache，然后定期写入磁盘，aof会每个一段时间写数据到os cache，定期调用操作系统的fsync操作，将OS cache中数据刷入磁盘文件中，aof会导致日志持久化文件越来越大，当大到一定的时候，- AOF会做rewrite操作，rewrite操作会基于当时内存中的数据来构造一个更小的AOF文件，然后将旧的文件删除，此时新的AOF文件会丢失掉被LRU算法淘汰掉的那部分数据，redis会有限定的内存大小，到达最大值时，会使用LRU算法淘汰掉一部分数据</code></pre><h3 id="AOF的工作流程"><a href="#AOF的工作流程" class="headerlink" title="AOF的工作流程"></a>AOF的工作流程</h3><pre><code>1.redis fork一个子进程2.子进程基于当前内存中的数据，构建日志，开始向一个新的临时的AOF文件中写日志3.redis主进程接收到client新的写操作之后，在内存中写入日志，同时新的日志也继续写入旧的aof文件4.子进程写完新的日志文件之后，主进程将内存中的新日志再次追加到新的AOF文件中5.用新的日志文件替换掉旧的日志文件</code></pre><h3 id="AOF-配置"><a href="#AOF-配置" class="headerlink" title="AOF 配置"></a>AOF 配置</h3><pre><code>- 要开启 AOF 持久化方式，需要在配置文件中将 appendonly 修改为 yes。- appendfilename ：aof文件名，默认是&quot;appendonly.aof&quot;- appendfsync：aof持久化策略的配置；  1.no表示不执行fsync，由操作系统保证数据同步到磁盘，速度最快，但是不太安全；  2.always表示每次写入都执行fsync，以保证数据同步到磁盘，效率很低；  3.everysec表示每秒执行一次fsync，可能会导致丢失这1s数据。通常选择 everysec ，兼顾安全性和效率。</code></pre><h3 id="AOF-优点"><a href="#AOF-优点" class="headerlink" title="AOF 优点"></a>AOF 优点</h3><pre><code>1.可以更好的保护数据不丢失，一般AOF会每隔1秒钟，通过一个后台线程执行一次fsync操作，最多丢失1秒的数据2.AOF以append-only模式写入，所以没有磁盘寻址开销，写入性能非常高，而且文件不易破损，即使破损，也容易修复，破损也是文件尾部破损，redis提供有工具修复3.AOF文件即使过大，出现后台重写操作，也不会影响客户端的读写，因为rewrite log时，会进行压缩，创建出一份需要恢复的最小日志出来，在创建新日志的时候，老的日志文件还是照常写入,当新的merge后的日志文件ready的时候，在交换新老日志文件即可4.AOF日志文件通过非常可读的方式进行记录，此特性适合做灾难性的误删除的紧急恢复，比如用flushall清空了所有数据，只要此时后台rewrite还没有发生，就可以立即拷贝AOF文件，将最后一条执行的flushall命令删除，再将AOF日志文件放回去，通过恢复机制，自动恢复所有数据。</code></pre><h3 id="AOF-缺点"><a href="#AOF-缺点" class="headerlink" title="AOF 缺点"></a>AOF 缺点</h3><pre><code>1.对于同一份数据来说，AOF文件通常比RDB文件更大2.AOF开启后，支持的QPS会比RDB低，因为AOF一般配置每1秒fsync一次日志文件3.AOF发生过bug，进行数据恢复时，没有恢复一模一样的数据，类似AOF这种较为复杂的基于命令日志/merge/回放的方式，比基于RDB每次持久化一份数据的方式更脆弱一些，AOF为了避免rewrite过程导致的bug，每次rewrite并不是基于旧的指令日志进行merge的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性更好些。4.数据恢复会比较慢</code></pre><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><pre><code>1.AOF也可以用于做冷备份，每隔一定时间去copy这份数据2.rdb做冷备份是由redis控制生成快照文件，AOF要写脚本，定时任务才能实现    3.RDB快照生成间隔周期要短，否则丢失的数据多，恢复时对客户端影响也大（数据大会导致客户端服务暂停一段时间）4.如果保证一条数据不丢，可以将fsync设置成每写一个命令，就fsync一次，但此时redis的QPS会大幅降低5.如果想让redis仅仅当做纯内存的缓存来使用，那么可以禁止RDB和AOF的持久化机制6.通过持久化机制，可以把redis内存中的数据持久化到磁盘上，然后可以将这些数据备份到别的地方去，比如阿里云7.如果同时使用了RDB和AOF两种持久化机制，那么在redis重启的时候，会使用AOF来重新构建数据，因为AOF中的数据更完整</code></pre><h2 id="redis企业级备份方案"><a href="#redis企业级备份方案" class="headerlink" title="redis企业级备份方案"></a>redis企业级备份方案</h2><pre><code>- 数据备份方案：    1.写定时任务脚本    2.每小时copy一份rdb到一个目录，仅仅保留最近48小时的备份    3.每天都保留一份当日的备份，保留最近一个月的备份    4.每次copy备份的时候，把旧备份删除    5.每天晚上将当前服务器上所有的数据备份发送一份到远程的云服务上去- 数据恢复：    1.如果是redis进程挂掉，重启redis，会基于aof恢复数据    2.如果redis所在的机器挂掉，重启机器，重启redis    3.如果最新的rdb和aof文件出现丢失或破损，可以基于当前机器最新的rdb数据副本进行恢复    4.如果有重大数据错误，比如程序存在bug，将redis数据污染了，可以选择某个更早的时间点，对数据进行恢复- 使用rdb数据备份恢复数据时，要关闭aof功能，否则，redis会基于aof恢复数据，而不是rdb</code></pre><blockquote><p>参考文档：<br/><br>1.<a href="https://www.cnblogs.com/ysocean/p/9114267.html" target="_blank" rel="noopener">https://www.cnblogs.com/ysocean/p/9114267.html</a> <br/><br>2.<a href="https://redis.io/topics/persistence" target="_blank" rel="noopener">https://redis.io/topics/persistence</a></p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入剖析kubernetes系列之一</title>
      <link href="/2019/09/29/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90kubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E4%B8%80/"/>
      <url>/2019/09/29/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90kubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>  从即日起开写本系列，此系列是依据极客时间《深入剖析kubernetes》专栏整理的笔记，本系列会持续更新。</p><h3 id="第一篇-容器技术基础之进程"><a href="#第一篇-容器技术基础之进程" class="headerlink" title="第一篇    容器技术基础之进程"></a>第一篇    容器技术基础之进程</h3><ul><li><p>进程可以看作是一个程序运行起来后的计算机执行环境的总和。</p></li><li><p>cgroup技术是用来制造约束的主要手段，用于限制资源。</p></li><li><p>namespace是用来修改进程视图的主要方法，用于实现对应资源等的隔离。</p></li><li><p>容器docker里面最开始运行的进程，是PID=1的进程，这个1号进程只是容器中虚拟出来的，在宿主机系统进程中的pid不是1，例如为100，这是被pid namespace机制做了隔离的结果，，使得容器内进程的pid被施了障眼法，看不到前面的99个进程，而自己就是pid=1。</p></li><li><p>Linux系统提供的namespace有：mount，uts，ipc，network，user，pid</p></li><li><p>docker实际上是在创建容器进程时，指定了一组namespace参数，容器只能看到当前namespace所限定的资源等，容器其实是一种特殊的进程</p></li><li><p>容器是单进程的，容器镜像里面集成了jdk，netstat，ping等，容器启动时里面java进程在运行，但在容器里面执行ping命令时，此时ping命令是不受docker控制的，所以单进程不是只能运行一个进程，而是只有一个进程是可控的，这里的可控是指exec进去之后启动的进程，不受控制，控制指它们的回收和生命周期管理，pod中的prestop，poststart是pid=1的进程的子进程，不能把pod的hook写成后台进程，因为容器中只有pid=1的进程是容器可控的。</p></li><li><p>镜像只是提供了一套镜像文件系统中的各种文件，而各种内核相关的模块或者特性支持，完全依赖于宿主机</p></li><li><p>虚拟机与容器的区别： 虚拟机是硬件级别（虚拟硬件）的隔离，而容器是进程的隔离</p></li><li><p>通过查看/proc/1/cgroup下的文件结构，可以判断目前是在容器里面还是在宿主机里</p></li><li><p>监控容器内jvm使用情况，可以使用一个sidecar容器来监控主容器内tomcat的jmx，并将其数据转换成prometheus拉取的格式</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用开源软件的正确姿势</title>
      <link href="/2019/06/27/%E4%BD%BF%E7%94%A8%E5%BC%80%E6%BA%90%E8%BD%AF%E4%BB%B6%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF/"/>
      <url>/2019/06/27/%E4%BD%BF%E7%94%A8%E5%BC%80%E6%BA%90%E8%BD%AF%E4%BB%B6%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF/</url>
      
        <content type="html"><![CDATA[<h2 id="使用开源软件的正确姿势"><a href="#使用开源软件的正确姿势" class="headerlink" title="使用开源软件的正确姿势"></a>使用开源软件的正确姿势</h2><h3 id="一、选择开源软件的考量点："><a href="#一、选择开源软件的考量点：" class="headerlink" title="一、选择开源软件的考量点："></a>一、选择开源软件的考量点：</h3><ul><li>是否满足业务</li><li>是否成熟</li><li>可以从以下几个方面考察是否成熟：<ul><li>版本号：一般建议除非特殊情况，否则不要选0.X版本的，至少选1.X版本的，版本号越高越好。</li><li>使用的公司数量：一般开源项目都会把采用了自己项目的公司列在主页上，公司越大越好，数量越多越好。</li><li>社区活跃度：看看社区是否活跃，发帖数、回复数、问题处理速度等。</li></ul></li><li>运维能力</li><li>可以从以下几个方案去考察运维能力：<ul><li>开源方案日志是否齐全：有的开源方案日志只有寥寥启动停止几行，出了问题根本无法排查。</li><li>开源方案是否有命令行、管理控制台等维护工具，能够看到系统运行时的情况。</li><li>开源方案是否有故障检测和恢复的能力，例如告警、倒换等。</li></ul></li></ul><h3 id="二、使用开源软件的考量点："><a href="#二、使用开源软件的考量点：" class="headerlink" title="二、使用开源软件的考量点："></a>二、使用开源软件的考量点：</h3><ul><li><p>深入研究，仔细测试</p></li><li><p>可以从如下几方面进行研究和测试：</p><ul><li>通读开源项目的设计文档或者白皮书，了解其设计原理；</li><li>核对每个配置项的作用和影响，识别出关键配置项；</li><li>进行多种场景的性能测试；</li><li>进行压力测试，连续跑几天，观察cpu、内存、磁盘io等指标波动；</li><li>进行故障测试：kill，断电、拔网线、重启100次以上、倒换等。</li></ul></li><li><p>小心应用，灰度发布</p><ul><li>再怎么深入的研究，再怎么仔细的测试，都只能降低风险，但不可能完全覆盖所有线上场景。</li><li>先在非核心的业务上用，然后有经验后慢慢扩展。</li></ul></li><li><p>做好应急，以防万一</p><ul><li>对于重要的业务或者数据，使用开源项目时，最好有另外一个比较成熟的方案做备份，尤其是数据存储。例如：如果要用MongoDB或者Redis，可以用MySQL做备份存储。这样做虽然复杂度和成本高一些，但关键时刻能够救命！</li></ul></li></ul><h3 id="三、基于开源项目做开发的考量点："><a href="#三、基于开源项目做开发的考量点：" class="headerlink" title="三、基于开源项目做开发的考量点："></a>三、基于开源项目做开发的考量点：</h3><ul><li><p>保持纯洁，加以包装</p><ul><li><p>不要改动原系统，而是要开发辅助系统: 监控，报警，负载均衡，管理等。</p></li><li><p>给开源项目提需求或者bug</p></li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>虚拟机磁盘误删除恢复故障</title>
      <link href="/2019/06/20/%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E8%AF%AF%E5%88%A0%E9%99%A4%E6%81%A2%E5%A4%8D/"/>
      <url>/2019/06/20/%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E8%AF%AF%E5%88%A0%E9%99%A4%E6%81%A2%E5%A4%8D/</url>
      
        <content type="html"><![CDATA[<ul><li><p>最近一台存放重要文件的Windows7虚拟机磁盘文件被误删除，导致虚拟机无法启动，只留下两个文件，test-flat.vmdk和test-000001-delta.vmdk,启动虚拟机时报错，信息如下：the file specified is not a virtual disk。无法打开磁盘“/vmfs/volumes4db4f346-a928774c-50af-3c4a92731f32/test/,test-flat.vmdk”或其所依赖的快照磁盘之一。</p></li><li><p>看到此景，这可咋办，上网查阅相关资料，终于找到了解决办法.</p></li><li><p>flat.vmdk文件：这是个默认虚拟磁盘数据文件，创建于你添加虚拟硬盘驱动到虚拟机时，而不是RDM。当使用厚磁盘时，这个文件的大小相当于你创建虚拟硬盘驱动时所指定的大小。</p></li><li><p>delta.vmdk文件：这个虚拟磁盘数据文件只用于创建虚拟机快照时。当创建了快照，对原始flat.vmdk的所有写入都停止，并变成只读；然后这些对虚拟磁盘的更改将写入delta文件。这些文件的初始大小是16MB，然后随着对虚拟机虚拟硬盘的更改需要而以16MB的速度增长。因为这些文件是虚拟磁盘所作更改的位图，一个单一delta.vmdk文件不能超过原始flat.vmdk文件的大小。每为虚拟机创建一个快照就会生成一个delta文件，并且它们的文件名以数字递增（如test-000001-delta.vmdk, test-000002-delta.vmdk）。当快照融合到原始–flat.vmdk文件后再删除时，这些文件将自动删除。</p></li><li><p>综上所述，可以确定数据都还在，开始进行数据恢复工作,步骤如下：</p></li></ul><pre class=" language-bash"><code class="language-bash">1.创建一台同样配置的虚拟机，不创建磁盘，命名虚拟机名字为test1.2.ssh登录到esxi宿主机，默认ssh服务没开启，首先开启ssh服务，具体方法可从网络上搜索，ssh登录，结果悲剧了，不能密码登录，被前任负责人设置了秘钥登录，找不到秘钥了，唉！！！！算了，秘钥问题等过后在解决，先使用client工具登录vcenter服务器，转移test-flat.vmdk和test-000001-delta.vmdk至另外一台esxi服务器的一台新建虚拟机<span class="token punctuation">(</span>创建时不创建磁盘<span class="token punctuation">)</span>中.3.开启ssh服务，登录至此esxi宿主机，查找test-flat.vmdk所在的目录并进入其所在目录中<span class="token comment" spellcheck="true"># find / -name 'test-flat.vmdk</span><span class="token comment" spellcheck="true"># ls -la *vmdk   查看vmdk文件大小，下面要用到(FILE_SIZE)</span><span class="token comment" spellcheck="true"># vmkfstools -c FILE_SIZE -a lsilogic test1-flat.vmdk</span></code></pre><ul><li><p>启动虚拟机，ok，成功启动，进去查看要找的重要文件，发现是7月份的，不是最新的，7月份做过一次快照，此时的文件内容就是做快照之前的内容，做快照之后到故障期间的文件内容还没找回来，好在存在test-000001-delta.vmdk，这个文件内存储了快照之后的变化的文件内容，查看大小在5G左右</p></li><li><p>虚拟机关机，新创建一个快照，将test-000001-delta.vmdk重命名，覆盖新创建的快照的数据文件（delta.vmdk.文件）</p></li><li><p>开机启动虚拟机，发现要找的文件都在，已经成功全部恢复</p></li></ul><p>参考文档:(<a href="http://www.cnblogs.com/sammyliu/p/5661085.html" target="_blank" rel="noopener">http://www.cnblogs.com/sammyliu/p/5661085.html</a>)</p>]]></content>
      
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
